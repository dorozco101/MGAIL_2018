{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Policy Gradients (DDPG and REINFORCE)\n",
    "\n",
    "Name:David Orozco\n",
    "\n",
    "ID: A12054079"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "This exercise requires you to solve various continous control problems in OpenAI-Gym.  \n",
    "\n",
    "DDPG is policy gradient actor critic method for continous control which is off policy. It tackles the curse of dimensionality / loss of performance faced when discretizing a continous action domain. DDPG uses similiar \"tricks\" as DQN to improve the stability of training, including a replay buffer and target networks.\n",
    "\n",
    "Furthermore, you will implement REINFORCE for discrete and continous environments, and as a bonus compare the sample efficiency and performance with DQN and DDPG.\n",
    "\n",
    "\n",
    "### DDPG paper: https://arxiv.org/pdf/1509.02971.pdf\n",
    "\n",
    "### Environments:\n",
    "\n",
    "#### InvertedPendulum-v2 environment:\n",
    "<img src=\"inverted_pendulum.png\" width=\"300\">\n",
    "\n",
    "#### Pendulum-v0 environment:\n",
    "<img src=\"pendulum.png\" width=\"300\">\n",
    "\n",
    "#### Halfcheetah-v2 environment:\n",
    "<img src=\"half_cheetah.png\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment for Actor Critic\n",
    "- inline plotting\n",
    "- gym\n",
    "- directory for logging videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "#environment\n",
    "import gym\n",
    "import statistics\n",
    "#pytorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "logging_interval = 100\n",
    "animate_interval = logging_interval\n",
    "logdir='./DDPG/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up gym environment\n",
    "The code below does the following for you:\n",
    "- Wrap environment, log videos, setup CUDA variables (if GPU is available)\n",
    "- Record action and observation space dimensions\n",
    "- Fix random seed for determinisitic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-30 20:29:37,368] Making new env: HalfCheetah-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ENV: HalfCheetah-v1\n",
      "obs space: 17\n",
      "act dim: 6\n",
      "[ 0.09762701  0.43037873  0.20552675  0.08976637 -0.1526904   0.29178823]\n"
     ]
    }
   ],
   "source": [
    "envName = 'cheetah'\n",
    "VISUALIZE = False\n",
    "SEED = 0\n",
    "MAX_PATH_LENGTH = 500\n",
    "NUM_EPISODES = 12000\n",
    "GAMMA=0.99\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Environments to be tested on\n",
    "if envName == 'inverted pendulum':\n",
    "    env_name = 'InvertedPendulum-v1'\n",
    "elif envName == 'pendulum':\n",
    "    env_name = 'Pendulum-v0'\n",
    "elif envName =='cheetah':\n",
    "    env_name = 'HalfCheetah-v1' \n",
    "elif envName == 'cartpole':\n",
    "    env_name='CartPole-v0'\n",
    "else:\n",
    "    env_name = envName\n",
    "    \n",
    "print(\"Using ENV: \"+env_name)\n",
    "# wrap gym to save videos\n",
    "envOriginal = gym.make(env_name)\n",
    "env=envOriginal\n",
    "if VISUALIZE:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = gym.wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%logging_interval==0)\n",
    "env._max_episodes_steps = MAX_PATH_LENGTH\n",
    "\n",
    "# check observation and action space\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "if discrete:\n",
    "    act_dim = int(env.action_space.n /2)\n",
    "else:\n",
    "    act_dim = env.action_space.shape[0]\n",
    "print (\"obs space: \"+str(obs_dim))\n",
    "print(\"act dim: \"+str(act_dim))\n",
    "#print (env.action_space.high)\n",
    "if discrete:\n",
    "    print(\"This is a discrete action space, probably not the right algorithm to use\")\n",
    "print((env.action_space.sample()))\n",
    "env.reset()\n",
    "env.step(env.action_space.sample())\n",
    "# set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# make variable types for automatic setting to GPU or CPU, depending on GPU availability\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate your understanding of the simulation:\n",
    "For the environments mentioned above ('Pendulum-v0', 'HalfCheetah-v2', 'InvertedPendulum-v2'),\n",
    "- describe the reward system\n",
    "- describe the each state variable (observation space)\n",
    "- describe the action space\n",
    "- when is the environment considered \"solved\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: For 'Half Cheetah v1' the reward system is based both on the energy used to move the cheetah (i.e. torque applied is negative reward) and positive reward is the speed at which the cheetah moves forward. The observation space is 17 variables which represent the joints position, velocity(slider), angle(hinge), and angular velocity (hinge). The action space is a 6 dimensional vector that represents the actuator torques that act on the hinge joints of the cheetah. (3 for the front 3 joints, and 3 for the back 3 joints). The environment is done when the cheetah reaches a time limit of 1000.\n",
    "\n",
    "For 'Pendulum v1' there is one dimension for action space which is a continuous value ranging from -2 to 2 and represents the joint effort applied to the pendelum. There is 3 dimensional observation space which represents functions of the theta (pendulum angle) cos(thet),sin(thet)..etc. This is a continuous state space as we would expect. The reward system is highest for an angle of theta = 0 (upright pendulum), thus we want to remain in this position. There is no specific termination condition since it is a time dependent termination.\n",
    "\n",
    "For 'InvertedPendulum v1' there is a 4 dimensional observation space which is similar to cartpole, representing angles and velocities.  The action space is one dimensional which is a continous number repesenting the cart moving left or right. The game is over when the cart leaves the frame or the pole falls below a certain angle. Reward is the same as cart pole which is to have the pole up at an angle of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an action normalization class:\n",
    "To train across various environments, it is useful to normalize action inputs and outputs between [-1, 1]. This class should take in actions and implement forward and reverse functions to map actions between [-1, 1] and [action_space.low, action_space.high].\n",
    "\n",
    "Using the following gym wrapper, implement this class.\n",
    "- https://github.com/openai/gym/blob/78c416ef7bc829ce55b404b6604641ba0cf47d10/gym/core.py\n",
    "- i.e. we are overriding the outputs scale of actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeAction(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        #tanh outputs (-1,1) from tanh, need to be [action_space.low, action_space.high]\n",
    "        Range = (env.action_space.high-env.action_space.low)\n",
    "        action = ((action + 1.)*(Range/2.))+env.action_space.low\n",
    "      \n",
    "        return action\n",
    "    def reverse_action(self, action):\n",
    "        #reverse of that above\n",
    "        Range = (env.action_space.high-env.action_space.low)\n",
    "        action = (action - env.action_space.low)/Range\n",
    "        action = (action*2.)-1.\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a weight syncing function\n",
    "In contrast to DQN, DDPG uses soft weight sychronization. At each time step following training, the actor and critic target network weights are updated to track the rollout networks. \n",
    "- target_network.weights <= target_network.weights \\* (1 - tau) + source_network.weights \\* (tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightSync(target_model, source_model, tau = 0.001):\n",
    "    for parameter_target, parameter_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        parameter_target.data.copy_((1 - tau) * parameter_target.data + tau * parameter_source.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Replay class that includes all the functionality of a replay buffer\n",
    "DDPG is an off policy actor-critic method and an identical replay buffer to that used for the previous assignment is applicable here as well (do not include the generate_minibatch method in your Replay class this time). Like before, your constructor for Replay should create an initial buffer of size 1000 when you instantiate it.\n",
    "\n",
    "The replay buffer should kept to some maximum size (60000), allow adding of samples and returning of samples at random from the buffer. Each sample (or experience) is formed as (state, action, reward, next_state, done). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay:\n",
    "    \n",
    "    def __init__(self,max_size=60000):\n",
    "        self.max_size = max_size\n",
    "        self.replay_buffer = []\n",
    "        \n",
    "    def initialize(self,env,init_length = 1000):\n",
    "        observation = env.reset()\n",
    "        for steps in range(init_length):\n",
    "            #time.sleep(0.05)\n",
    "            #env.render()\n",
    "            experience = []\n",
    "            observation = np.reshape(observation, [1, observation.size])\n",
    "            #append state\n",
    "            experience.append(observation)\n",
    "            action = env.action_space.sample()\n",
    "            #append random action\n",
    "            \n",
    "            \n",
    "            observation,reward,done,_= env.step(action)\n",
    "            \n",
    "            if type(action) != int:\n",
    "                action = np.reshape(action, [1, action.size])\n",
    "            experience.append(action)\n",
    "            #append reward\n",
    "            experience.append(reward)\n",
    "            observation = np.reshape(observation, [1, observation.size])\n",
    "            #append next state\n",
    "            experience.append(observation)\n",
    "            #append done flag\n",
    "            experience.append(done)\n",
    "            self.replay_buffer.append(experience)\n",
    "           \n",
    "            if done == True:\n",
    "                observation = env.reset()\n",
    "                \n",
    "    def add_memory(self,memory):\n",
    "        if len(self.replay_buffer) > self.max_size:\n",
    "            self.replay_buffer.pop(0)\n",
    "            self.replay_buffer.append(memory)\n",
    "        else:\n",
    "            self.replay_buffer.append(memory)\n",
    "        \n",
    "    def return_samples(self,numSamples):\n",
    "        batch = random.sample(self.replay_buffer,numSamples)\n",
    "        return batch\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write an Ornstein Uhlenbeck process class for exploration noise\n",
    "The proccess is described here:\n",
    "- https://en.wikipedia.org/wiki/Ornsteinâ€“Uhlenbeck_process\n",
    "- http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "\n",
    "You should implement:\n",
    "- a step / sample method\n",
    "- reset method\n",
    "\n",
    "Use theta = 0.15, mu = 0, sigma = 0.3, dt = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckProcess():\n",
    "    def __init__(self,dimension):\n",
    "        #piazza values: theta=0.25, mu=0, sigma=0.05, dt=0.01\n",
    "        self.th = 0.25\n",
    "        self.mu = 0\n",
    "        self.sig = 0.05\n",
    "        self.dt = 0.01\n",
    "        self.dimension = dimension\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        # Time vector\n",
    "        self.x = np.zeros((1,self.dimension)) # Allocate output vector, set initial condition\n",
    "        # Set random seed ?\n",
    "    def step(self):\n",
    "        self.x = self.x+self.th*(self.mu-self.x)*self.dt+self.sig*np.sqrt(self.dt)*np.random.normal(self.mu, self.sig, (1,self.dimension) )\n",
    "        return self.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Deep Neural Network class that creates a dense network of a desired architecture for actor and critic networks\n",
    "\n",
    "\n",
    "#### Actor\n",
    "- input and hidden layer activation function: ReLU\n",
    "\n",
    "- output activation function: Tanh\n",
    "\n",
    "- hidden_state sizes: 400\n",
    "\n",
    "- state and action sizes: variable\n",
    "\n",
    "- number of hidden layers: 2\n",
    "\n",
    "- batch normalization applied to all hidden layers\n",
    "\n",
    "- weight initialization: normal distribution with small variance. \n",
    "\n",
    "#### Critic\n",
    "- input and hidden layer activation function: ReLU\n",
    "\n",
    "- output activation function: None\n",
    "\n",
    "- hidden_state sizes: 300, 300 + action size\n",
    "\n",
    "- state and action sizes: variable\n",
    "\n",
    "- number of hidden layers: 2\n",
    "\n",
    "- batch normalization applied to all hidden layers prior to the action input\n",
    "\n",
    "- weight initialization: normal distribution with small variance.\n",
    "\n",
    "Good baselines can be found in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# actor model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 2 hidden layers, 400 units per layer, tanh output to bound outputs between -1 and 1\n",
    "\n",
    "class actor(nn.Module):\n",
    "    def __init__(self,input_size, output_size,use_batchNorm = True):\n",
    "        super(actor, self).__init__()\n",
    "        self.hidden_layers = 2\n",
    "        self.hidden_units = 400\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, self.hidden_units,bias=True)\n",
    "        #torch.nn.init.normal(self.fc1.weight,0,0.1)\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_units)\n",
    "        torch.nn.init.xavier_uniform(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.hidden_units, self.hidden_units,bias=True)\n",
    "        #torch.nn.init.normal(self.fc2.weight,0,0.1)\n",
    "        self.bn2 = nn.BatchNorm1d(self.hidden_units)\n",
    "        torch.nn.init.xavier_uniform(self.fc2.weight)\n",
    "        \n",
    "        self.fc3 = nn.Linear(self.hidden_units, output_size,bias=True)\n",
    "        #torch.nn.init.normal(self.fc3.weight,0,0.1)\n",
    "        torch.nn.init.xavier_uniform(self.fc3.weight)\n",
    "        self.use_BN = use_batchNorm\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.use_BN:\n",
    "            x = F.relu(self.bn1(self.fc1(x)))\n",
    "            x = F.relu(self.bn2(self.fc2(x)))\n",
    "            x = F.tanh(self.fc3(x))\n",
    "        else:\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.tanh(self.fc3(x))\n",
    "        #x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "# ----------------------------------------------------\n",
    "# critic model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 2 hidden layers, 300 units per layer, ouputs rewards therefore unbounded\n",
    "# Action not to be included until 2nd layer of critic (from paper). Make sure to formulate your critic.forward() accordingly\n",
    "\n",
    "class critic(nn.Module):\n",
    "    def __init__(self,input_size,action_size, output_size,use_batchNorm = True):\n",
    "        super(critic, self).__init__()\n",
    "        self.hidden_layers = 2\n",
    "        self.hidden_units = 300\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, self.hidden_units,bias=True)\n",
    "        #torch.nn.init.normal(self.fc1.weight,0,0.1)\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_units)\n",
    "        torch.nn.init.xavier_uniform(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.hidden_units+action_size, self.hidden_units,bias=True)\n",
    "        #torch.nn.init.normal(self.fc2.weight,0,0.1)\n",
    "        torch.nn.init.xavier_uniform(self.fc2.weight)\n",
    "        \n",
    "        self.fc3 = nn.Linear(self.hidden_units, output_size,bias=True)\n",
    "        torch.nn.init.normal(self.fc3.weight,0,0.1)\n",
    "        #torch.nn.init.xavier_uniform(self.fc3.weight)\n",
    "        self.use_BN = use_batchNorm\n",
    "        \n",
    "        \n",
    "    def forward(self, x, action):\n",
    "        if self.use_BN:\n",
    "            x = F.relu(self.bn1(self.fc1(x)))\n",
    "            x = torch.cat((x,action),1)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "        else:\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = torch.cat((x,action),1)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DDPG class to encapsulate definition, rollouts, and training\n",
    "\n",
    "- gamma = 0.99\n",
    "\n",
    "- actor_lr = 1e-4\n",
    "\n",
    "- critic_lr = 1e-3\n",
    "\n",
    "- critic l2 regularization = 1e-2\n",
    "\n",
    "- noise decay\n",
    "\n",
    "- noise class\n",
    "\n",
    "- batch_size = 128\n",
    "\n",
    "- optimizer: Adam\n",
    "\n",
    "- loss (critic): mse\n",
    "\n",
    "Furthermore, you can experiment with action versus parameter space noise. The standard implimentation works with action space noise, howeve parameter space noise has shown to produce excellent results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(state, action, reward, next_state, done)\n",
    "class DDPG:\n",
    "    def __init__(self, obs_dim, act_dim, critic_lr = 1e-3, actor_lr = 1e-4, gamma = 0.99, batch_size = 128,use_BN = True):\n",
    "        \n",
    "        self.gamma = GAMMA\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        \n",
    "        # actor\n",
    "        self.actor = actor(input_size = obs_dim, output_size = act_dim, use_batchNorm = use_BN).type(FloatTensor)\n",
    "        self.actor_target = actor(input_size = obs_dim, output_size = act_dim,use_batchNorm = use_BN).type(FloatTensor)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        # critic\n",
    "        self.critic = critic(input_size = obs_dim, action_size = act_dim, output_size = 1,use_batchNorm = use_BN).type(FloatTensor)\n",
    "        self.critic_target = critic(input_size = obs_dim, action_size = act_dim, output_size = 1,use_batchNorm = use_BN).type(FloatTensor)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        # optimizers\n",
    "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr = actor_lr)\n",
    "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr = critic_lr, weight_decay=1e-2)\n",
    "        \n",
    "        # critic loss\n",
    "        self.critic_loss = nn.MSELoss()\n",
    "        \n",
    "        # noise\n",
    "        self.noise = OrnsteinUhlenbeckProcess(dimension = act_dim)\n",
    "\n",
    "        # replay buffer \n",
    "        self.replayBuffer = Replay()\n",
    "        \n",
    "        self.state_size = obs_dim\n",
    "        self.action_size = act_dim\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def train(self):\n",
    "     \n",
    "        # sample from Replay\n",
    "        samples  = self.replayBuffer.return_samples(BATCH_SIZE)\n",
    "        # update critic (create target for Q function)\n",
    "        rewards = np.zeros((len(samples),1))\n",
    "        next_states = np.zeros((len(samples),self.state_size))\n",
    "        current_states = np.zeros((len(samples),self.state_size))\n",
    "        current_actions = np.zeros((len(samples),self.action_size))\n",
    "        \n",
    "        for index,sample in enumerate(samples):\n",
    "            rewards[index] = sample[2] #reward\n",
    "            next_states[index,:] = sample[3]\n",
    "            current_states[index,:] = sample[0]\n",
    "            current_actions[index,:] = sample[1]\n",
    "            \n",
    "        current_states = Variable(torch.from_numpy(current_states).float().cuda())\n",
    "        current_actions = Variable(torch.from_numpy(current_actions).float().cuda())\n",
    "        nextState = Variable(torch.from_numpy(next_states).float().cuda())\n",
    "        \n",
    "        actions = self.actor_target(nextState)\n",
    "        critic_targets=rewards+(self.gamma*self.critic_target(nextState,actions).cpu().detach().numpy())\n",
    "        # critic optimizer and backprop step (feed in target and predicted values to self.critic_loss)\n",
    "        #print(np.shape(critic_targets))\n",
    "        \n",
    "        target =  Variable(torch.from_numpy(critic_targets).float().cuda())\n",
    "        output = self.critic(current_states,current_actions)\n",
    "        loss = self.critic_loss(output,target)\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        # update actor (formulate the loss wrt which actor is updated)\n",
    "        actor_loss = -torch.mean(self.critic(current_states,self.actor(current_states)))\n",
    "        # actor optimizer and backprop step (loss_actor.backward())\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        # sychronize target network with fast moving one\n",
    "        weightSync(self.critic_target, self.critic)\n",
    "        weightSync(self.actor_target, self.actor)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of your DDPG object\n",
    "- Print network architectures, confirm they are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  from ipykernel import kernelapp as app\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor(\n",
      "  (fc1): Linear(in_features=17, out_features=400, bias=True)\n",
      "  (bn1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (bn2): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=400, out_features=6, bias=True)\n",
      ")\n",
      "critic(\n",
      "  (fc1): Linear(in_features=17, out_features=300, bias=True)\n",
      "  (bn1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=306, out_features=300, bias=True)\n",
      "  (fc3): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:60: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    }
   ],
   "source": [
    "if envName == 'pendulum':\n",
    "    use_batchNorm = False\n",
    "    print('not using BN')\n",
    "else:\n",
    "    use_batchNorm = True\n",
    "    \n",
    "ddpg = DDPG(obs_dim = obs_dim, act_dim = act_dim, use_BN = use_batchNorm)\n",
    "ddpg.replayBuffer.initialize(env)\n",
    "print(ddpg.actor)\n",
    "print(ddpg.critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-30 20:40:50,487] Making new env: HalfCheetah-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ENV: HalfCheetah-v1\n",
      "obs space: 17\n",
      "act dim: 6\n",
      "[-0.08160615  0.88097931 -0.29010504 -0.65402865 -0.05581189  0.30518324]\n"
     ]
    }
   ],
   "source": [
    "VISUALIZE = False\n",
    "SEED = 0\n",
    "MAX_PATH_LENGTH = 500\n",
    "NUM_EPISODES = 20000\n",
    "GAMMA=0.99\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Environments to be tested on\n",
    "if envName == 'inverted pendulum':\n",
    "    env_name = 'InvertedPendulum-v1'\n",
    "elif envName == 'pendulum':\n",
    "    env_name = 'Pendulum-v0'\n",
    "elif envName == 'chetah':\n",
    "    env_name = 'HalfCheetah-v1' \n",
    "elif envName == 'cartpole':\n",
    "    env_name='CartPole-v0'\n",
    "    \n",
    "print(\"Using ENV: \"+env_name)\n",
    "# wrap gym to save videos\n",
    "envOriginal = gym.make(env_name)\n",
    "env=envOriginal\n",
    "if VISUALIZE:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = gym.wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%logging_interval==0)\n",
    "env._max_episodes_steps = MAX_PATH_LENGTH\n",
    "\n",
    "# check observation and action space\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = int(env.action_space.n) if discrete else env.action_space.shape[0]\n",
    "print (\"obs space: \"+str(obs_dim))\n",
    "print(\"act dim: \"+str(act_dim))\n",
    "#print (env.action_space.high)\n",
    "if discrete:\n",
    "    print(\"This is a discrete action space, probably not the right algorithm to use\")\n",
    "print((env.action_space.sample()))\n",
    "#env.reset()\n",
    "#env.step(env.action_space.sample())\n",
    "# set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# make variable types for automatic setting to GPU or CPU, depending on GPU availability\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DDPG on different environments\n",
    "Early stopping conditions:\n",
    "- avg_val > 500 for \"InvertedPendulum\" \n",
    "- avg_val > -150 for \"Pendulum\" \n",
    "- avg_val > 1500 for \"HalfCheetah\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr: 0count: 1000\n",
      "Average value: 125.1727260257824 Reward:125.1727260257824 for episode: 0\n",
      "Itr: 1count: 2000\n",
      "Average value: 122.67455589478075 Reward:75.20932340574979 for episode: 1\n",
      "Itr: 2count: 3000\n",
      "Average value: 121.19470787097057 Reward:93.07759541857746 for episode: 2\n",
      "Itr: 3count: 4000\n",
      "Average value: 119.12591222587896 Reward:79.8187949691386 for episode: 3\n",
      "Itr: 4count: 5000\n",
      "Average value: 120.49176202260641 Reward:146.4429081604279 for episode: 4\n",
      "Itr: 5count: 6000\n",
      "Average value: 121.53672066452363 Reward:141.39093486095067 for episode: 5\n",
      "Itr: 6count: 7000\n",
      "Average value: 124.71882851716774 Reward:185.17887771740595 for episode: 6\n",
      "Itr: 7count: 8000\n",
      "Average value: 128.76380579798038 Reward:205.6183741334207 for episode: 7\n",
      "Itr: 8count: 9000\n",
      "Average value: 132.7242683874216 Reward:207.97305758680494 for episode: 8\n",
      "Itr: 9count: 10000\n",
      "Average value: 135.13446787126935 Reward:180.92825806437688 for episode: 9\n",
      "Itr: 10count: 11000\n",
      "Average value: 140.46012434503638 Reward:241.64759734660984 for episode: 10\n",
      "Itr: 11count: 12000\n",
      "Average value: 132.5683750746168 Reward:-17.37486106335483 for episode: 11\n",
      "Itr: 12count: 13000\n",
      "Average value: 140.7981617430233 Reward:297.16410844274657 for episode: 12\n",
      "Itr: 13count: 14000\n",
      "Average value: 150.05709832032193 Reward:325.97689328899617 for episode: 13\n",
      "Itr: 14count: 15000\n",
      "Average value: 153.48852445832833 Reward:218.68562108045026 for episode: 14\n",
      "Itr: 15count: 16000\n",
      "Average value: 155.09433775714666 Reward:185.60479043469496 for episode: 15\n",
      "Itr: 16count: 17000\n",
      "Average value: 154.6583520254663 Reward:146.3746231235394 for episode: 16\n",
      "Itr: 17count: 18000\n",
      "Average value: 158.78741531240516 Reward:237.2396177642438 for episode: 17\n",
      "Itr: 18count: 19000\n",
      "Average value: 164.74401697375788 Reward:277.9194485394593 for episode: 18\n",
      "Itr: 19count: 20000\n",
      "Average value: 166.38958413947066 Reward:197.65536028801364 for episode: 19\n",
      "Itr: 20count: 21000\n",
      "Average value: 170.61142147531254 Reward:250.8263308563082 for episode: 20\n",
      "Itr: 21count: 22000\n",
      "Average value: 173.78490609154284 Reward:234.08111379991834 for episode: 21\n",
      "Itr: 22count: 23000\n",
      "Average value: 182.77823763781242 Reward:353.651537016935 for episode: 22\n",
      "Itr: 23count: 24000\n",
      "Average value: 185.02000386641106 Reward:227.6135622097852 for episode: 23\n",
      "Itr: 24count: 25000\n",
      "Average value: 196.52680495791455 Reward:415.15602569648087 for episode: 24\n",
      "Itr: 25count: 26000\n",
      "Average value: 206.55332740941344 Reward:397.0572539878923 for episode: 25\n",
      "Itr: 26count: 27000\n",
      "Average value: 215.54950510548105 Reward:386.4768813307659 for episode: 26\n",
      "Itr: 27count: 28000\n",
      "Average value: 227.14504842287403 Reward:447.4603714533404 for episode: 27\n",
      "Itr: 28count: 29000\n",
      "Average value: 227.85956812167683 Reward:241.43544239893 for episode: 28\n",
      "Itr: 29count: 30000\n",
      "Average value: 237.19758152298226 Reward:414.6198361477851 for episode: 29\n",
      "Itr: 30count: 31000\n",
      "Average value: 245.53335495022634 Reward:403.9130500678638 for episode: 30\n",
      "Itr: 31count: 32000\n",
      "Average value: 246.93549405149136 Reward:273.5761369755272 for episode: 31\n",
      "Itr: 32count: 33000\n",
      "Average value: 246.77945492285926 Reward:243.81471147884926 for episode: 32\n",
      "Itr: 33count: 34000\n",
      "Average value: 233.9498992144295 Reward:-9.811659245735253 for episode: 33\n",
      "Itr: 34count: 35000\n",
      "Average value: 245.58479976171765 Reward:466.6479101601925 for episode: 34\n",
      "Itr: 35count: 36000\n",
      "Average value: 254.7109017037713 Reward:428.1068386027907 for episode: 35\n",
      "Itr: 36count: 37000\n",
      "Average value: 255.8923638687819 Reward:278.34014500398354 for episode: 36\n",
      "Itr: 37count: 38000\n",
      "Average value: 234.22965860219665 Reward:-177.36174146292257 for episode: 37\n",
      "Itr: 38count: 39000\n",
      "Average value: 240.29215652770273 Reward:355.4796171123185 for episode: 38\n",
      "Itr: 39count: 40000\n",
      "Average value: 244.65982412106538 Reward:327.6455083949559 for episode: 39\n",
      "Itr: 40count: 41000\n",
      "Average value: 254.09543695127624 Reward:433.3720807252826 for episode: 40\n",
      "Itr: 41count: 42000\n",
      "Average value: 259.23020143122113 Reward:356.7907265501746 for episode: 41\n",
      "Itr: 42count: 43000\n",
      "Average value: 267.27247395193405 Reward:420.07565184548 for episode: 42\n",
      "Itr: 43count: 44000\n",
      "Average value: 275.8956302386885 Reward:439.7355996870237 for episode: 43\n",
      "Itr: 44count: 45000\n",
      "Average value: 283.3769922380987 Reward:425.52287022689285 for episode: 44\n",
      "Itr: 45count: 46000\n",
      "Average value: 240.5185397295948 Reward:-573.7920579319788 for episode: 45\n",
      "Itr: 46count: 47000\n",
      "Average value: 247.3413254817094 Reward:376.974254771887 for episode: 46\n",
      "Itr: 47count: 48000\n",
      "Average value: 251.96185921482424 Reward:339.7520001440066 for episode: 47\n",
      "Itr: 48count: 49000\n",
      "Average value: 256.16019413071456 Reward:335.9285575326309 for episode: 48\n",
      "Itr: 49count: 50000\n",
      "Average value: 261.0327178891433 Reward:353.61066929928984 for episode: 49\n",
      "Itr: 50count: 51000\n",
      "Average value: 265.9604890505895 Reward:359.5881411180674 for episode: 50\n",
      "Itr: 51count: 52000\n",
      "Average value: 271.3995647519507 Reward:374.7420030778142 for episode: 51\n",
      "Itr: 52count: 53000\n",
      "Average value: 274.9225245228378 Reward:341.8587601696921 for episode: 52\n",
      "Itr: 53count: 54000\n",
      "Average value: 277.7622700588161 Reward:331.7174352424036 for episode: 53\n",
      "Itr: 54count: 55000\n",
      "Average value: 278.4925719756591 Reward:292.3683083956757 for episode: 54\n",
      "Itr: 55count: 56000\n",
      "Average value: 237.3970640680755 Reward:-543.4175861760116 for episode: 55\n",
      "Itr: 56count: 57000\n",
      "Average value: 238.02455589884957 Reward:249.94690068355666 for episode: 56\n",
      "Itr: 57count: 58000\n",
      "Average value: 240.896345030298 Reward:295.4603385278182 for episode: 57\n",
      "Itr: 58count: 59000\n",
      "Average value: 241.7113319375938 Reward:257.1960831762139 for episode: 58\n",
      "Itr: 59count: 60000\n",
      "Average value: 239.62937724641566 Reward:200.07223811403102 for episode: 59\n",
      "Itr: 60count: 61000\n",
      "Average value: 242.7860676601875 Reward:302.7631855218527 for episode: 60\n",
      "Itr: 61count: 62000\n",
      "Average value: 205.42212675463838 Reward:-504.4927504507949 for episode: 61\n",
      "Itr: 62count: 63000\n",
      "Average value: 207.9802187961386 Reward:256.58396758464283 for episode: 62\n",
      "Itr: 63count: 64000\n",
      "Average value: 211.8641726025366 Reward:285.65929492409884 for episode: 63\n",
      "Itr: 64count: 65000\n",
      "Average value: 218.09934780253144 Reward:336.5676766024339 for episode: 64\n",
      "Itr: 65count: 66000\n",
      "Average value: 188.57407352402632 Reward:-372.40613776757084 for episode: 65\n",
      "Itr: 66count: 67000\n",
      "Average value: 150.32609953041887 Reward:-576.3854063481223 for episode: 66\n",
      "Itr: 67count: 68000\n",
      "Average value: 119.93275285399189 Reward:-457.5408339981206 for episode: 67\n",
      "Itr: 68count: 69000\n",
      "Average value: 85.76837895235008 Reward:-563.3547251788442 for episode: 68\n",
      "Itr: 69count: 70000\n",
      "Average value: 51.708128302913295 Reward:-595.4366340363856 for episode: 69\n",
      "Itr: 70count: 71000\n",
      "Average value: 29.350410877203178 Reward:-395.4462202112889 for episode: 70\n",
      "Itr: 71count: 72000\n",
      "Average value: 2.616974296793007 Reward:-505.3183207310002 for episode: 71\n",
      "Itr: 72count: 73000\n",
      "Average value: -8.779707294924846 Reward:-225.31665753756403 for episode: 72\n",
      "Itr: 73count: 74000\n",
      "Average value: 16.796528426325697 Reward:502.745007130086 for episode: 73\n",
      "Itr: 74count: 75000\n",
      "Average value: 60.03060301950375 Reward:881.4780202898867 for episode: 74\n",
      "Itr: 75count: 76000\n",
      "Average value: 71.03164118303926 Reward:280.05136629021376 for episode: 75\n",
      "Itr: 76count: 77000\n",
      "Average value: 90.8948758025831 Reward:468.29633357391634 for episode: 76\n",
      "Itr: 77count: 78000\n",
      "Average value: 117.75647264485315 Reward:628.1268126479843 for episode: 77\n",
      "Itr: 78count: 79000\n",
      "Average value: 156.61735028471074 Reward:894.9740254420051 for episode: 78\n",
      "Itr: 79count: 80000\n",
      "Average value: 181.76847142994006 Reward:659.639773189297 for episode: 79\n",
      "Itr: 80count: 81000\n",
      "Average value: 208.3740614493397 Reward:713.8802718179327 for episode: 80\n",
      "Itr: 81count: 82000\n",
      "Average value: 239.88672651460615 Reward:838.627362754669 for episode: 81\n",
      "Itr: 82count: 83000\n",
      "Average value: 269.5495239250615 Reward:833.1426747237138 for episode: 82\n",
      "Itr: 83count: 84000\n",
      "Average value: 299.2087362039408 Reward:862.7337695026471 for episode: 83\n",
      "Itr: 84count: 85000\n",
      "Average value: 324.5233097081256 Reward:805.5002062876367 for episode: 84\n",
      "Itr: 85count: 86000\n",
      "Average value: 345.4251323061349 Reward:742.5597616683116 for episode: 85\n",
      "Itr: 86count: 87000\n",
      "Average value: 371.8151374462231 Reward:873.2252351078987 for episode: 86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr: 87count: 88000\n",
      "Average value: 395.13109268960324 Reward:838.1342423138254 for episode: 87\n",
      "Itr: 88count: 89000\n",
      "Average value: 418.621499817043 Reward:864.9392352383986 for episode: 88\n",
      "Itr: 89count: 90000\n",
      "Average value: 442.0803348171973 Reward:887.7981998201292 for episode: 89\n",
      "Itr: 90count: 91000\n",
      "Average value: 458.8302046935579 Reward:777.0777323444099 for episode: 90\n",
      "Itr: 91count: 92000\n",
      "Average value: 484.8941859857293 Reward:980.1098305369859 for episode: 91\n",
      "Itr: 92count: 93000\n",
      "Average value: 504.99824516512894 Reward:886.9753695737234 for episode: 92\n",
      "Itr: 93count: 94000\n",
      "Average value: 524.6409130055588 Reward:897.8516019737274 for episode: 93\n",
      "Itr: 94count: 95000\n",
      "Average value: 540.4959554733454 Reward:841.7417623612906 for episode: 94\n",
      "Itr: 95count: 96000\n",
      "Average value: 557.345538208973 Reward:877.4876101858995 for episode: 95\n",
      "Itr: 96count: 97000\n",
      "Average value: 573.1712944745365 Reward:873.8606635202425 for episode: 96\n",
      "Itr: 97count: 98000\n",
      "Average value: 585.6737680117501 Reward:823.2207652188098 for episode: 97\n",
      "Itr: 98count: 99000\n",
      "Average value: 594.5841766319048 Reward:763.8819404148438 for episode: 98\n",
      "Itr: 99count: 100000\n",
      "Average value: 610.153725013362 Reward:905.9751442610498 for episode: 99\n",
      "Itr: 100count: 101000\n",
      "Average value: 628.5240613788865 Reward:977.5604523238525 for episode: 100\n",
      "Itr: 101count: 102000\n",
      "Average value: 643.2342226568842 Reward:922.7272869388414 for episode: 101\n",
      "Itr: 102count: 103000\n",
      "Average value: 661.0298853267041 Reward:999.1474760532835 for episode: 102\n",
      "Itr: 103count: 104000\n",
      "Average value: 676.6733954264625 Reward:973.9000873218748 for episode: 103\n",
      "Itr: 104count: 105000\n",
      "Average value: 692.2616857656545 Reward:988.4392022103015 for episode: 104\n",
      "Itr: 105count: 106000\n",
      "Average value: 704.9194509563679 Reward:945.4169895799205 for episode: 105\n",
      "Itr: 106count: 107000\n",
      "Average value: 711.8959486442315 Reward:844.4494047136384 for episode: 106\n",
      "Itr: 107count: 108000\n",
      "Average value: 700.8334983170597 Reward:490.6469421007967 for episode: 107\n",
      "Itr: 108count: 109000\n",
      "Average value: 702.9784384426933 Reward:743.7323008297327 for episode: 108\n",
      "Itr: 109count: 110000\n",
      "Average value: 714.082279589949 Reward:925.0552613878082 for episode: 109\n",
      "Itr: 110count: 111000\n",
      "Average value: 727.9618241387037 Reward:991.673170565043 for episode: 110\n",
      "Itr: 111count: 112000\n",
      "Average value: 739.8259480439899 Reward:965.2443022444274 for episode: 111\n",
      "Itr: 112count: 113000\n",
      "Average value: 752.5245048911542 Reward:993.797084987274 for episode: 112\n",
      "Itr: 113count: 114000\n",
      "Average value: 764.0234865318955 Reward:982.5041377059817 for episode: 113\n",
      "Itr: 114count: 115000\n",
      "Average value: 766.9889271540537 Reward:823.332298975061 for episode: 114\n",
      "Itr: 115count: 116000\n",
      "Average value: 773.3959021103454 Reward:895.1284262798882 for episode: 115\n",
      "Itr: 116count: 117000\n",
      "Average value: 781.1620315212506 Reward:928.718490328448 for episode: 116\n",
      "Itr: 117count: 118000\n",
      "Average value: 793.1621067805397 Reward:1021.1635367070342 for episode: 117\n",
      "Itr: 118count: 119000\n",
      "Average value: 803.8582572878631 Reward:1007.0851169270072 for episode: 118\n",
      "Itr: 119count: 120000\n",
      "Average value: 816.3917268881052 Reward:1054.5276492927032 for episode: 119\n",
      "Itr: 120count: 121000\n",
      "Average value: 824.7431875114405 Reward:983.4209393548113 for episode: 120\n",
      "Itr: 121count: 122000\n",
      "Average value: 834.4687484645938 Reward:1019.2544065745082 for episode: 121\n",
      "Itr: 122count: 123000\n",
      "Average value: 820.4697011916966 Reward:554.4878030066493 for episode: 122\n",
      "Itr: 123count: 124000\n",
      "Average value: 833.0692001383494 Reward:1072.4596801247553 for episode: 123\n",
      "Itr: 124count: 125000\n",
      "Average value: 844.6967857816478 Reward:1065.620913004317 for episode: 124\n",
      "Itr: 125count: 126000\n",
      "Average value: 856.6733937332925 Reward:1084.2289448145418 for episode: 125\n",
      "Itr: 126count: 127000\n",
      "Average value: 871.0849211415327 Reward:1144.9039418980956 for episode: 126\n",
      "Itr: 127count: 128000\n",
      "Average value: 837.293735866272 Reward:195.2612156363168 for episode: 127\n",
      "Itr: 128count: 129000\n",
      "Average value: 793.7130639091275 Reward:-34.31970327661718 for episode: 128\n",
      "Itr: 129count: 130000\n",
      "Average value: 808.5641583003564 Reward:1090.7349517337063 for episode: 129\n",
      "Itr: 130count: 131000\n",
      "Average value: 840.3467404645851 Reward:1444.2158015849293 for episode: 130\n",
      "Itr: 131count: 132000\n",
      "Average value: 772.1391445217706 Reward:-523.8051783917039 for episode: 131\n",
      "Itr: 132count: 133000\n",
      "Average value: 706.3204844375115 Reward:-544.2340571634119 for episode: 132\n",
      "Itr: 133count: 134000\n",
      "Average value: 641.3882168435783 Reward:-592.3248674411498 for episode: 133\n",
      "Itr: 134count: 135000\n",
      "Average value: 579.5759816867062 Reward:-594.8564862938642 for episode: 134\n",
      "Itr: 135count: 136000\n",
      "Average value: 520.8587764856834 Reward:-594.7681223337474 for episode: 135\n",
      "Itr: 136count: 137000\n",
      "Average value: 465.1515258367096 Reward:-593.2862364937918 for episode: 136\n",
      "Itr: 137count: 138000\n",
      "Average value: 412.18689044264846 Reward:-594.1411820445121 for episode: 137\n",
      "Itr: 138count: 139000\n",
      "Average value: 472.11831135272166 Reward:1610.8153086441134 for episode: 138\n",
      "Itr: 139count: 140000\n",
      "Average value: 529.5027214691455 Reward:1619.8065136811995 for episode: 139\n",
      "Itr: 140count: 141000\n",
      "Average value: 584.1836144533289 Reward:1623.1205811528134 for episode: 140\n",
      "Itr: 141count: 142000\n",
      "Average value: 634.6183584176011 Reward:1592.8784937387732 for episode: 141\n",
      "Itr: 142count: 143000\n",
      "Average value: 682.8457364776483 Reward:1599.165919618544 for episode: 142\n",
      "Itr: 143count: 144000\n",
      "Average value: 723.3742326377593 Reward:1493.415659679869 for episode: 143\n",
      "Itr: 144count: 145000\n",
      "Average value: 759.8750239951614 Reward:1453.390059785803 for episode: 144\n",
      "Itr: 145count: 146000\n",
      "Average value: 790.7335652668036 Reward:1377.045849428004 for episode: 145\n",
      "Itr: 146count: 147000\n",
      "Average value: 820.8223168231273 Reward:1392.508596393279 for episode: 146\n",
      "Itr: 147count: 148000\n",
      "Average value: 849.8385405600886 Reward:1401.146791562354 for episode: 147\n",
      "Itr: 148count: 149000\n",
      "Average value: 876.3090738707813 Reward:1379.2492067739427 for episode: 148\n",
      "Itr: 149count: 150000\n",
      "Average value: 904.3053303270178 Reward:1436.234202995514 for episode: 149\n",
      "Itr: 150count: 151000\n",
      "Average value: 927.8834453363248 Reward:1375.8676305131585 for episode: 150\n",
      "Itr: 151count: 152000\n",
      "Average value: 954.4859253607543 Reward:1459.933045824917 for episode: 151\n",
      "Itr: 152count: 153000\n",
      "Average value: 983.0785273442588 Reward:1526.3379650308434 for episode: 152\n",
      "Itr: 153count: 154000\n",
      "Average value: 1005.8911844956767 Reward:1439.3316703726184 for episode: 153\n",
      "Itr: 154count: 155000\n",
      "Average value: 1036.0096065340629 Reward:1608.2596252633973 for episode: 154\n",
      "Itr: 155count: 156000\n",
      "Average value: 1064.2366818465077 Reward:1600.5511127829616 for episode: 155\n",
      "Itr: 156count: 157000\n",
      "Average value: 1091.7419556375673 Reward:1614.3421576677001 for episode: 156\n",
      "Itr: 157count: 158000\n",
      "Average value: 1117.5390628117816 Reward:1607.68409912185 for episode: 157\n",
      "Itr: 158count: 159000\n",
      "Average value: 1146.4096868976037 Reward:1694.9515445282273 for episode: 158\n",
      "Itr: 159count: 160000\n",
      "Average value: 1172.3774756670875 Reward:1665.7654622872801 for episode: 159\n",
      "Itr: 160count: 161000\n",
      "Average value: 1194.9393512950685 Reward:1623.6149882267089 for episode: 160\n",
      "Itr: 161count: 162000\n",
      "Average value: 1211.0976845128166 Reward:1518.1060156500337 for episode: 161\n",
      "Itr: 162count: 163000\n",
      "Average value: 1227.9008440972718 Reward:1547.1608762019232 for episode: 162\n",
      "Itr: 163count: 164000\n",
      "Average value: 1246.3407457728786 Reward:1596.698877609408 for episode: 163\n",
      "Itr: 164count: 165000\n",
      "Average value: 1267.4824158851027 Reward:1669.174148017364 for episode: 164\n",
      "Itr: 165count: 166000\n",
      "Average value: 1285.9327456972262 Reward:1636.4890121275703 for episode: 165\n",
      "Itr: 166count: 167000\n",
      "Average value: 1303.7993387490283 Reward:1643.2646067332666 for episode: 166\n",
      "Itr: 167count: 168000\n",
      "Average value: 1322.427698824497 Reward:1676.3665402584063 for episode: 167\n",
      "Itr: 168count: 169000\n",
      "Average value: 1341.1392763738256 Reward:1696.6592498110736 for episode: 168\n",
      "Itr: 169count: 170000\n",
      "Average value: 1359.609447600378 Reward:1710.5427009048756 for episode: 169\n",
      "Itr: 170count: 171000\n",
      "Average value: 1377.6790705256851 Reward:1721.0019061065232 for episode: 170\n",
      "Itr: 171count: 172000\n",
      "Average value: 1396.44946394792 Reward:1753.086938970388 for episode: 171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr: 172count: 173000\n",
      "Average value: 1411.9167520970468 Reward:1705.7952269304521 for episode: 172\n",
      "Itr: 173count: 174000\n",
      "Average value: 1436.2237362287754 Reward:1898.0564347316197 for episode: 173\n",
      "Itr: 174count: 175000\n",
      "Average value: 1456.3274677031445 Reward:1838.2983657161603 for episode: 174\n",
      "Itr: 175count: 176000\n",
      "Average value: 1480.3944473306328 Reward:1937.667060252909 for episode: 175\n",
      "Itr: 176count: 177000\n",
      "Average value: 1506.4241443520316 Reward:2000.9883877586085 for episode: 176\n",
      "Itr: 177count: 178000\n",
      "Average value: 1526.7289029795584 Reward:1912.5193169025677 for episode: 177\n",
      "Itr: 178count: 179000\n",
      "Average value: 1555.7313032529128 Reward:2106.776908446643 for episode: 178\n",
      "Itr: 179count: 180000\n",
      "Average value: 1580.3492899234068 Reward:2048.091036662795 for episode: 179\n",
      "Itr: 180count: 181000\n",
      "Average value: 1605.0672887423868 Reward:2074.7092663030066 for episode: 180\n",
      "Itr: 181count: 182000\n",
      "Average value: 1628.0604648537658 Reward:2064.930810969967 for episode: 181\n",
      "Itr: 182count: 183000\n",
      "Average value: 1644.6265635455227 Reward:1959.3824386889044 for episode: 182\n",
      "Itr: 183count: 184000\n",
      "Average value: 1659.9219246006376 Reward:1950.5337846478237 for episode: 183\n",
      "Itr: 184count: 185000\n",
      "Average value: 1682.4572676814114 Reward:2110.6287862161175 for episode: 184\n",
      "Itr: 185count: 186000\n",
      "Average value: 1706.2615303576397 Reward:2158.5425212059768 for episode: 185\n",
      "Itr: 186count: 187000\n",
      "Average value: 1731.55107743886 Reward:2212.0524719820446 for episode: 186\n",
      "Itr: 187count: 188000\n",
      "Average value: 1763.2637357249566 Reward:2365.804243160796 for episode: 187\n",
      "Itr: 188count: 189000\n",
      "Average value: 1793.7902062826 Reward:2373.793146877826 for episode: 188\n",
      "Itr: 189count: 190000\n",
      "Average value: 1825.1684560999659 Reward:2421.355202629921 for episode: 189\n",
      "Itr: 190count: 191000\n",
      "Average value: 1855.5159246399762 Reward:2432.117826900178 for episode: 190\n",
      "Itr: 191count: 192000\n",
      "Average value: 1877.2174938458886 Reward:2289.547308758226 for episode: 191\n",
      "Itr: 192count: 193000\n",
      "Average value: 1909.818548142868 Reward:2529.23857978548 for episode: 192\n",
      "Itr: 193count: 194000\n",
      "Average value: 1935.6576388254946 Reward:2426.6003617954025 for episode: 193\n",
      "Itr: 194count: 195000\n",
      "Average value: 1938.1186817998896 Reward:1984.878498313397 for episode: 194\n",
      "Itr: 195count: 196000\n",
      "Average value: 1968.453265725965 Reward:2544.8103603214 for episode: 195\n",
      "Itr: 196count: 197000\n",
      "Average value: 1990.3179973070185 Reward:2405.747897347037 for episode: 196\n",
      "Itr: 197count: 198000\n",
      "Average value: 2013.2609774526484 Reward:2449.1776002196184 for episode: 197\n",
      "Itr: 198count: 199000\n",
      "Average value: 2035.4005468347134 Reward:2456.052365093953 for episode: 198\n",
      "Itr: 199count: 200000\n",
      "Average value: 2022.3818789679551 Reward:1775.0271894995526 for episode: 199\n",
      "Itr: 200count: 201000\n",
      "Average value: 2020.9973753940212 Reward:1994.6918074892817 for episode: 200\n",
      "Itr: 201count: 202000\n",
      "Average value: 2046.6680539243853 Reward:2534.4109460013015 for episode: 201\n",
      "Itr: 202count: 203000\n",
      "Average value: 2064.075967982577 Reward:2394.826335088221 for episode: 202\n",
      "Itr: 203count: 204000\n",
      "Average value: 2079.397295952615 Reward:2370.502527383344 for episode: 203\n",
      "Itr: 204count: 205000\n",
      "Average value: 2096.6605760413804 Reward:2424.6628977279292 for episode: 204\n",
      "Itr: 205count: 206000\n",
      "Average value: 2117.816672501579 Reward:2519.782505245349 for episode: 205\n",
      "Itr: 206count: 207000\n",
      "Average value: 2130.2654496897035 Reward:2366.7922162640684 for episode: 206\n",
      "Itr: 207count: 208000\n",
      "Average value: 2146.1016794099455 Reward:2446.9900440945485 for episode: 207\n",
      "Itr: 208count: 209000\n",
      "Average value: 2159.8365698928897 Reward:2420.799489068833 for episode: 208\n",
      "Itr: 209count: 210000\n",
      "Average value: 2169.5855818148284 Reward:2354.8168083316677 for episode: 209\n",
      "Itr: 210count: 211000\n",
      "Average value: 2175.294023488336 Reward:2283.7544152849787 for episode: 210\n",
      "Itr: 211count: 212000\n",
      "Average value: 2187.358478565249 Reward:2416.583125026603 for episode: 211\n",
      "Itr: 212count: 213000\n",
      "Average value: 2203.3843692995197 Reward:2507.876293250667 for episode: 212\n",
      "Itr: 213count: 214000\n",
      "Average value: 2216.710423795662 Reward:2469.90545922236 for episode: 213\n",
      "Itr: 214count: 215000\n",
      "Average value: 2228.361212394123 Reward:2449.7261957648884 for episode: 214\n",
      "Itr: 215count: 216000\n",
      "Average value: 2238.5358180775224 Reward:2431.8533260621216 for episode: 215\n",
      "Itr: 216count: 217000\n",
      "Average value: 2244.1423275876705 Reward:2350.6660082804865 for episode: 216\n",
      "Itr: 217count: 218000\n",
      "Average value: 2258.3650739070144 Reward:2528.5972539745444 for episode: 217\n",
      "Itr: 218count: 219000\n",
      "Average value: 2273.0157217958895 Reward:2551.378031684516 for episode: 218\n",
      "Itr: 219count: 220000\n",
      "Average value: 2184.66720946323 Reward:506.0454751427046 for episode: 219\n",
      "Itr: 220count: 221000\n",
      "Average value: 2198.472169437604 Reward:2460.7664089507107 for episode: 220\n",
      "Itr: 221count: 222000\n",
      "Average value: 2204.2554216392145 Reward:2314.1372134698236 for episode: 221\n",
      "Itr: 222count: 223000\n",
      "Average value: 2209.901350068129 Reward:2317.1739902175095 for episode: 222\n",
      "Itr: 223count: 224000\n",
      "Average value: 2222.13647812427 Reward:2454.6039111909568 for episode: 223\n",
      "Itr: 224count: 225000\n",
      "Average value: 2235.8136319911296 Reward:2495.6795554614678 for episode: 224\n",
      "Itr: 225count: 226000\n",
      "Average value: 2252.432804740688 Reward:2568.1970869823012 for episode: 225\n",
      "Itr: 226count: 227000\n",
      "Average value: 2260.1825491521827 Reward:2407.427692970588 for episode: 226\n",
      "Itr: 227count: 228000\n",
      "Average value: 2273.2893504358594 Reward:2522.318574825723 for episode: 227\n",
      "Itr: 228count: 229000\n",
      "Average value: 2279.2539378127185 Reward:2392.5810979730436 for episode: 228\n",
      "Itr: 229count: 230000\n",
      "Average value: 2282.4640880738025 Reward:2343.456943034398 for episode: 229\n",
      "Itr: 230count: 231000\n",
      "Average value: 2280.457834411668 Reward:2242.339014831115 for episode: 230\n",
      "Itr: 231count: 232000\n",
      "Average value: 2283.7886824603156 Reward:2347.074795384622 for episode: 231\n",
      "Itr: 232count: 233000\n",
      "Average value: 2291.281492087278 Reward:2433.6448749995643 for episode: 232\n",
      "Itr: 233count: 234000\n",
      "Average value: 2281.1898533068024 Reward:2089.448716477771 for episode: 233\n",
      "Itr: 234count: 235000\n",
      "Average value: 2293.7200155207984 Reward:2531.793097586726 for episode: 234\n",
      "Itr: 235count: 236000\n",
      "Average value: 2305.9431115661027 Reward:2538.1819364268886 for episode: 235\n",
      "Itr: 236count: 237000\n",
      "Average value: 2319.475118973193 Reward:2576.5832597079034 for episode: 236\n",
      "Itr: 237count: 238000\n",
      "Average value: 2333.439726504202 Reward:2598.767269593382 for episode: 237\n",
      "Itr: 238count: 239000\n",
      "Average value: 2347.877501339716 Reward:2622.1952232144804 for episode: 238\n",
      "Itr: 239count: 240000\n",
      "Average value: 2361.188537521918 Reward:2614.098224983752 for episode: 239\n",
      "Itr: 240count: 241000\n",
      "Average value: 2375.241739651057 Reward:2642.252580104692 for episode: 240\n",
      "Itr: 241count: 242000\n",
      "Average value: 2389.747634347841 Reward:2665.359633586738 for episode: 241\n",
      "Itr: 242count: 243000\n",
      "Average value: 2403.5652258065825 Reward:2666.0994635226652 for episode: 242\n",
      "Itr: 243count: 244000\n",
      "Average value: 2415.185149140658 Reward:2635.963692488099 for episode: 243\n",
      "Itr: 244count: 245000\n",
      "Average value: 2428.7927586794376 Reward:2687.337339916258 for episode: 244\n",
      "Itr: 245count: 246000\n",
      "Average value: 2433.1658683467795 Reward:2516.254952026273 for episode: 245\n",
      "Itr: 246count: 247000\n",
      "Average value: 2442.0357364427055 Reward:2610.5632302653016 for episode: 246\n",
      "Itr: 247count: 248000\n",
      "Average value: 2368.6365417697098 Reward:974.0518429827852 for episode: 247\n",
      "Itr: 248count: 249000\n",
      "Average value: 2374.4039318176738 Reward:2483.984342728989 for episode: 248\n",
      "Itr: 249count: 250000\n",
      "Average value: 2388.2609287505534 Reward:2651.5438704752723 for episode: 249\n",
      "Itr: 250count: 251000\n",
      "Average value: 2400.2796102181687 Reward:2628.6345581028622 for episode: 250\n",
      "Itr: 251count: 252000\n",
      "Average value: 2414.984433550886 Reward:2694.376076872512 for episode: 251\n",
      "Itr: 252count: 253000\n",
      "Average value: 2269.771168984012 Reward:-489.2808577865847 for episode: 252\n",
      "Itr: 253count: 254000\n",
      "Average value: 2137.8008787408476 Reward:-369.6346358792736 for episode: 253\n",
      "Itr: 254count: 255000\n",
      "Average value: 2009.2506253550491 Reward:-433.20418897511786 for episode: 254\n",
      "Itr: 255count: 256000\n",
      "Average value: 1878.0774636017154 Reward:-614.2126097116238 for episode: 255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr: 256count: 257000\n",
      "Average value: 1768.031780317606 Reward:-322.83620208047256 for episode: 256\n",
      "Itr: 257count: 258000\n",
      "Average value: 1664.2360671503761 Reward:-307.8824830269944 for episode: 257\n",
      "Itr: 258count: 259000\n",
      "Average value: 1694.3908730412272 Reward:2267.332184967399 for episode: 258\n",
      "Itr: 259count: 260000\n",
      "Average value: 1731.3484133092466 Reward:2433.5416784016143 for episode: 259\n",
      "Itr: 260count: 261000\n",
      "Average value: 1770.5652854937498 Reward:2515.6858569993115 for episode: 260\n",
      "Itr: 261count: 262000\n",
      "Average value: 1812.506185721628 Reward:2609.383290051319 for episode: 261\n",
      "Itr: 262count: 263000\n",
      "Average value: 1853.1338535043647 Reward:2625.0595413763645 for episode: 262\n",
      "Itr: 263count: 264000\n",
      "Average value: 1891.8349854681821 Reward:2627.156492780712 for episode: 263\n",
      "Itr: 264count: 265000\n",
      "Average value: 1922.0955994226756 Reward:2497.047264558057 for episode: 264\n",
      "Itr: 265count: 266000\n",
      "Average value: 1957.3960253904695 Reward:2628.1041187785545 for episode: 265\n",
      "Itr: 266count: 267000\n",
      "Average value: 1992.6884677811236 Reward:2663.244873203551 for episode: 266\n",
      "Itr: 267count: 268000\n",
      "Average value: 2025.8222171877044 Reward:2655.3634559127445 for episode: 267\n",
      "Itr: 268count: 269000\n",
      "Average value: 2050.131394374848 Reward:2512.005760930573 for episode: 268\n",
      "Itr: 269count: 270000\n",
      "Average value: 2078.2587930947234 Reward:2612.679368772363 for episode: 269\n",
      "Itr: 270count: 271000\n",
      "Average value: 2085.0906476150717 Reward:2214.8958835016897 for episode: 270\n",
      "Itr: 271count: 272000\n",
      "Average value: 2088.0770242934527 Reward:2144.8181811826944 for episode: 271\n",
      "Itr: 272count: 273000\n",
      "Average value: 2103.7350184167103 Reward:2401.236906758605 for episode: 272\n",
      "Itr: 273count: 274000\n",
      "Average value: 2123.3331804833 Reward:2495.698259748512 for episode: 273\n",
      "Itr: 274count: 275000\n",
      "Average value: 2006.6149992468547 Reward:-211.03044424560943 for episode: 274\n",
      "Itr: 275count: 276000\n",
      "Average value: 2034.6172380305568 Reward:2566.659774920899 for episode: 275\n",
      "Itr: 276count: 277000\n",
      "Average value: 2045.8756455746507 Reward:2259.7853889124394 for episode: 276\n",
      "Itr: 277count: 278000\n",
      "Average value: 2067.4795304586805 Reward:2477.953343255246 for episode: 277\n",
      "Itr: 278count: 279000\n",
      "Average value: 2091.7626178659107 Reward:2553.1412786032824 for episode: 278\n",
      "Itr: 279count: 280000\n",
      "Average value: 2111.551559676265 Reward:2487.5414540729903 for episode: 279\n",
      "Itr: 280count: 281000\n",
      "Average value: 2125.9216603060772 Reward:2398.953572272516 for episode: 280\n",
      "Itr: 281count: 282000\n",
      "Average value: 2141.2979506024726 Reward:2433.447466233992 for episode: 281\n",
      "Itr: 282count: 283000\n",
      "Average value: 2153.6893497461965 Reward:2389.1259334769557 for episode: 282\n",
      "Itr: 283count: 284000\n",
      "Average value: 2164.9064152325354 Reward:2378.0306594729796 for episode: 283\n",
      "Itr: 284count: 285000\n",
      "Average value: 2172.9296139231556 Reward:2325.3703890449406 for episode: 284\n",
      "Itr: 285count: 286000\n",
      "Average value: 2183.774829372281 Reward:2389.833922905672 for episode: 285\n",
      "Itr: 286count: 287000\n",
      "Average value: 2188.6062564755284 Reward:2280.4033714372354 for episode: 286\n",
      "Itr: 287count: 288000\n",
      "Average value: 2201.9325183333535 Reward:2455.1314936320323 for episode: 287\n",
      "Itr: 288count: 289000\n",
      "Average value: 2218.517300393905 Reward:2533.6281595443816 for episode: 288\n",
      "Itr: 289count: 290000\n",
      "Average value: 2230.127455265564 Reward:2450.720397827086 for episode: 289\n",
      "Itr: 290count: 291000\n",
      "Average value: 2242.4790813148056 Reward:2477.159976250397 for episode: 290\n",
      "Itr: 291count: 292000\n",
      "Average value: 2258.315050566095 Reward:2559.1984663405956 for episode: 291\n",
      "Itr: 292count: 293000\n",
      "Average value: 2270.4560905247945 Reward:2501.135849740084 for episode: 292\n",
      "Itr: 293count: 294000\n",
      "Average value: 2283.8882713899116 Reward:2539.0997078271394 for episode: 293\n",
      "Itr: 294count: 295000\n",
      "Average value: 2293.4615338624785 Reward:2475.3535208412513 for episode: 294\n",
      "Itr: 295count: 296000\n",
      "Average value: 2307.8848941053307 Reward:2581.9287387195263 for episode: 295\n",
      "Itr: 296count: 297000\n",
      "Average value: 2319.53282578673 Reward:2540.8435277333247 for episode: 296\n",
      "Itr: 297count: 298000\n",
      "Average value: 2333.8017565133346 Reward:2604.9114403188237 for episode: 297\n",
      "Itr: 298count: 299000\n",
      "Average value: 2344.3795338246186 Reward:2545.35730273902 for episode: 298\n",
      "Itr: 299count: 300000\n",
      "Average value: 2360.3666296053875 Reward:2664.121449440002 for episode: 299\n",
      "Itr: 300count: 301000\n",
      "breaking early : 2360.3666296053875\n",
      "Average value: 2373.3853058191826 for episode: 300\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "if not discrete:\n",
    "    env = NormalizeAction(env) # remap action values for the environment\n",
    "avg_val = 0\n",
    "\n",
    "#for plotting\n",
    "running_rewards_ddpg = []\n",
    "average_rewards_ddpg = []\n",
    "step_list_ddpg = []\n",
    "step_counter = 0\n",
    "done_recording = False\n",
    "# set term_condition for early stopping according to environment being used\n",
    "if envName =='pendulum':\n",
    "    term_condition = -150 # Pendulum\n",
    "elif envName == 'inverted pendulum':\n",
    "    term_condition = 500\n",
    "elif envName == 'cheetah':\n",
    "    term_condition = 1500\n",
    "elif envName == 'cartpole':\n",
    "    term_condition = 199\n",
    "else:\n",
    "    term_condition = 0\n",
    "    \n",
    "all_data = []\n",
    "for itr in range(NUM_EPISODES):\n",
    "    observation  = env.reset() # get initial state\n",
    "    #print(observation)\n",
    "    animate_this_episode = (itr % animate_interval == 0) and VISUALIZE\n",
    "    ddpg.noise.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    #print(ddpg.replayBuffer.replay_buffer)\n",
    "    #print(data1)\n",
    "    while True:\n",
    "        memory = []\n",
    "        observation = np.reshape(observation, [1, observation.size])\n",
    "        #all_data.append(observation.tolist())\n",
    "        memory.append(observation)\n",
    "        if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "                if avg_val > term_condition:\n",
    "                    done_recording = True\n",
    "\n",
    "        # use actor to get action, add ddpg.noise.step() to action\n",
    "        ddpg.actor.eval()\n",
    "        action = ddpg.actor(Variable(torch.from_numpy(observation).float().cuda()))\n",
    "        ddpg.actor.train()\n",
    "        #print(action.cpu().detach().numpy())\n",
    "        noise = ddpg.noise.step()\n",
    "        #print(noise)\n",
    "        action = action.cpu().detach().numpy()+noise\n",
    "        action = action[0]\n",
    "        \n",
    "        observation,reward,done,dual_rewards = env.step(action)\n",
    "        if type(action) != int:\n",
    "            action = np.reshape(action, [1,action.size])\n",
    "        memory.append(action)\n",
    "        \n",
    "        memory.append(reward)\n",
    "        observation = np.reshape(observation, [1, observation.size])\n",
    "        memory.append(observation)\n",
    "        memory.append(done)\n",
    "        ddpg.replayBuffer.add_memory(memory)\n",
    "        memory.append(env.env.env.data.qpos.flatten())\n",
    "        memory.append(env.env.env.data.qvel.flatten())\n",
    "        memory[0] = memory[0][0].tolist()\n",
    "        memory[2] = float(memory[2])\n",
    "        memory[1] = memory[1][0].tolist()\n",
    "        memory[3] = memory[3][0].tolist()\n",
    "        memory[4] = float(memory[4])\n",
    "        memory[5] = memory[5].tolist()\n",
    "        memory[6] = memory[6].tolist()\n",
    "\n",
    "        all_data.append(memory)\n",
    "        total_reward += reward\n",
    "        #print(observation)\n",
    "        # remember to put NN in eval mode while testing (to deal with BatchNorm layers) and put it back \n",
    "        # to train mode after you're done getting the action\n",
    "                \n",
    "        # step action, get next state, reward, done (keep track of total_reward)\n",
    "        # populate ddpg.replayBuffer\n",
    "        \n",
    "        ddpg.train()\n",
    "        \n",
    "        step_counter += 1\n",
    "        #if(step_counter%100 == 0):\n",
    "            #print(step_counter)\n",
    "        if done:\n",
    "            break\n",
    "    print(\"Itr: \"+str(itr)+\"count: \"+str(step_counter))\n",
    "\n",
    "    if avg_val > term_condition and itr > 100 and (done_recording or not VISUALIZE) and len(all_data) >300000:\n",
    "        print('breaking early : '+str(avg_val))\n",
    "        break\n",
    "\n",
    "    running_rewards_ddpg.append(total_reward) # return of this episode\n",
    "    average_rewards_ddpg.append(avg_val)\n",
    "    step_list_ddpg.append(step_counter)\n",
    "    if itr == 0:\n",
    "        avg_val = running_rewards_ddpg[-1]\n",
    "    else:\n",
    "        avg_val = avg_val * 0.95 + 0.05*running_rewards_ddpg[-1]\n",
    "    print(\"Average value: {} Reward:{} for episode: {}\".format(avg_val,running_rewards_ddpg[-1],itr))\n",
    "\n",
    "running_rewards_ddpg.append(total_reward) # return of this episode\n",
    "average_rewards_ddpg.append(avg_val)\n",
    "step_list_ddpg.append(step_counter)\n",
    "\n",
    "    \n",
    "output = open(env_name+'.pickle', 'wb')\n",
    "pickle.dump(all_data, output)\n",
    "output.close()\n",
    "avg_val = avg_val * 0.95 + 0.05*running_rewards_ddpg[-1]\n",
    "print(\"Average value: {} for episode: {}\".format(avg_val,itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60001\n",
      "721.0134550147966\n",
      "1039.5159081613788\n",
      "1092.1830903747061\n",
      "1175.6394479599294\n",
      "1153.0103731186675\n",
      "1206.2361348116672\n",
      "393.10217404380825\n",
      "-185.85947665950923\n",
      "194.19169841149485\n",
      "-304.38367401815236\n",
      "-333.2916427067197\n",
      "-435.7715750947921\n",
      "466.54060598457363\n",
      "773.7810934777822\n",
      "1338.942697129661\n",
      "1255.580222221099\n",
      "1236.3676815703336\n",
      "1402.1569857369416\n",
      "1360.9994564209903\n",
      "1425.8726740016975\n",
      "1322.2015633700269\n",
      "1465.9655319074361\n",
      "1435.2900947427784\n",
      "1397.9494287783452\n",
      "1412.4005923840487\n",
      "1304.9404091145698\n",
      "1493.5653413959626\n",
      "1374.4205725825095\n",
      "1411.6986383238634\n",
      "1333.6056517752306\n",
      "1408.8112799113655\n",
      "1551.780303745348\n",
      "1789.453449630853\n",
      "1494.412438670516\n",
      "1633.3303404994024\n",
      "1853.8809931611922\n",
      "1652.700899328849\n",
      "1848.471190839882\n",
      "1879.39990733072\n",
      "1851.8979698227647\n",
      "1849.6658780369662\n",
      "1912.726178496898\n",
      "1876.846889546164\n",
      "1862.795475403603\n",
      "1900.4779716792548\n",
      "1657.7836462771272\n",
      "1702.7632221107838\n",
      "1865.3299937727909\n",
      "1798.2080830165062\n",
      "1855.7041320340627\n",
      "1759.8814376715354\n",
      "1765.6059903943444\n",
      "1963.0578515463285\n",
      "1995.444044157828\n",
      "1881.7791608221926\n",
      "1993.709431077256\n",
      "1819.4377663994687\n",
      "1862.5560986883136\n",
      "1892.3031492285033\n",
      "1925.8326373551815\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "pkl_file = open('cheetah.pickle', 'rb')\n",
    "data1 = pickle.load(pkl_file)\n",
    "\n",
    "#print(data1[0])\n",
    "print(len(data1))\n",
    "episodes = []\n",
    "ep = []\n",
    "rewards = []\n",
    "reward = 0\n",
    "for i in range(len(data1)):\n",
    "    step = data1[i]\n",
    "    done = step[4]\n",
    "    reward+=step[2]\n",
    "    if not done:\n",
    "        ep.append(step)\n",
    "    else:\n",
    "        episodes.append(ep)\n",
    "        #print(len(ep))\n",
    "        print(reward)\n",
    "        ep = []\n",
    "        rewards.append(reward)\n",
    "        reward = 0\n",
    "        \n",
    "print(len(episodes))\n",
    "output = open('cheetah_episodes.pickle', 'wb')\n",
    "pickle.dump(episodes, output)\n",
    "output.close()\n",
    "    \n",
    "output = open('cheetah_total_rewards.pickle', 'wb')\n",
    "pickle.dump(rewards, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot rewards over multiple training runs \n",
    "This is provided to generate and plot results for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_ewma_vectorized_v2(data, window):\n",
    "\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure()\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg),20)\n",
    "#out = np.array(average_rew)\n",
    "step_list_ddpg = np.array(step_list_ddpg)\n",
    "plt.plot(step_list_ddpg, out)\n",
    "plt.title('Training reward: DDPG on env '+envName)\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "In this section you will implement REINFORCE, with modifications for batch training. It will be for use on both discrete and continous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Parametrization\n",
    "\n",
    "Define a MLP which outputs a distribution over the action preferences given input state. For the discrete case, the MLP outputs the likelihood of each action (softmax) while for the continuous case, the output is the mean and standard deviation parametrizing the normal distribution from which the action is sampled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Policy parametrizing model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 1 or 2 hidden layers with a small number of units per layer (similar to DQN)\n",
    "# use ReLU for hidden layer activations\n",
    "# softmax as activation for output if discrete actions, linear for continuous control\n",
    "# for the continuous case, output_dim=2*act_dim (each act_dim gets a mean and std_dev)\n",
    "\n",
    "class mlp(nn.Module):\n",
    "    def __init__(self,input_size, output_size,discrete):\n",
    "        super(mlp, self).__init__()\n",
    "        self.hidden_layers = 2\n",
    "        self.hidden_units = 40\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, self.hidden_units,bias=True)\n",
    "        #torch.nn.init.normal(self.fc1.weight,0,0.1)\n",
    "        #self.bn1 = nn.BatchNorm1d(self.hidden_units)\n",
    "        torch.nn.init.xavier_uniform(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.hidden_units, self.hidden_units,bias=True)\n",
    "        #torch.nn.init.normal(self.fc2.weight,0,0.1)\n",
    "        #self.bn2 = nn.BatchNorm1d(self.hidden_units)\n",
    "        torch.nn.init.xavier_uniform(self.fc2.weight)\n",
    "        if not discrete:\n",
    "            self.fc3 = nn.Linear(self.hidden_units, 2*output_size,bias=True)\n",
    "        elif discrete:\n",
    "            self.fc3 = nn.Linear(self.hidden_units, output_size,bias=True)\n",
    "        #torch.nn.init.normal(self.fc3.weight,0,0.1)\n",
    "        torch.nn.init.xavier_uniform(self.fc3.weight)\n",
    "        self.discrete = discrete\n",
    "        self.output_size = output_size\n",
    "        self.soft = nn.Softmax()\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        if self.discrete:\n",
    "            return self.soft(x)\n",
    "        else:\n",
    "            return torch.split(x, self.output_size, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that samples an action from the policy distribtion parameters obtained as output of the MLP. The function should return the action and the log-probability (log_odds) of taking that action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(logit, discrete):\n",
    "    # logit is the output of the softmax/linear layer\n",
    "    # discrete is a flag for the environment type\n",
    "    # Hint: use Categorical and Normal from torch.distributions to sample action and get the log-probability\n",
    "    # Note that log_probability in this case translates to ln(\\pi(a|s)) \n",
    "    if discrete:\n",
    "        m = torch.distributions.categorical.Categorical(logit)\n",
    "        action = m.sample()\n",
    "        #next_state, reward = env.step(action)\n",
    "        #loss = -m.log_prob(action) * reward\n",
    "        #loss.backward()\n",
    "        log_odds = m.log_prob(action)\n",
    "    else:\n",
    "        #print(logit)\n",
    "        m = torch.distributions.normal.Normal(logit[0][0],logit[1][0])\n",
    "        action = m.sample()\n",
    "        log_odds = m.log_prob(action)\n",
    "    return action, log_odds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function update_policy that defines the loss function and updates the MLP according to the REINFORCE update rule (ref. slide 24 of Lec 7 or page 330 of Sutton and Barto (2018)). The update algorithm to be used below is slightly different: instead of updating the network at every time-step, we take the gradient of the loss averaged over a batch of timesteps (this is to make SGD more stable). We also use a baseline to reduce variance. \n",
    "\n",
    "The discount factor is set as 1 here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(paths, net):\n",
    "    # paths: a list of paths (complete episodes, used to calculate return at each time step)\n",
    "    # net: MLP object\n",
    "\n",
    "    \n",
    "    num_paths = len(paths)\n",
    "    rew_cums = []\n",
    "    log_odds = []\n",
    "    gamma = 1\n",
    "    for path in paths:\n",
    "         # rew_cums should record return at each time step for each path \n",
    "        backwards = []\n",
    "        reward_sum = 0\n",
    "        rewards = path['reward']\n",
    "        for reward in reversed(rewards):\n",
    "            reward_sum  = gamma*reward_sum + reward\n",
    "            backwards.append(reward_sum)\n",
    "            \n",
    "        rew_cums = rew_cums + backwards[::-1]\n",
    "        log_odds = log_odds + path['log_odds']\n",
    "         # log_odds should record log_odds obtained at each timestep of path\n",
    "         # calculated as \"reward to go\"\n",
    "    rew_cums = np.asarray(rew_cums) \n",
    "    rew_cums = torch.autograd.Variable(torch.from_numpy(rew_cums)).type(FloatTensor)\n",
    "    rew_cums = (rew_cums - rew_cums.mean()) / (rew_cums.std() + 1e-5) # create baseline\n",
    "  #\n",
    "   # x = torch.from_numpy(rew_cums).float().cuda()\n",
    "    #print(x.size())\n",
    "    #print(len(log_odds))\n",
    "    log_odds = torch.stack(log_odds).squeeze().type(FloatTensor)\n",
    "    if len(log_odds.shape) > 1:\n",
    "        log_odds = log_odds.sum(dim=1)\n",
    "    loss = -1*torch.mean(torch.mul(rew_cums,log_odds))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # make log_odds, rew_cums each a vector\n",
    "        \n",
    "    # calculate policy loss and average over paths\n",
    "    \n",
    "    # take optimizer step\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment and instantiate objects. Your algorithm is to be tested on one discrete and two continuous environments. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:05:54,168] Making new env: CartPole-v0\n",
      "[2018-05-22 15:05:54,184] Finished writing results. You can upload them to the scoreboard via gym.upload('/datasets/home/86/786/dorozco/DDPG')\n",
      "[2018-05-22 15:05:54,193] Clearing 186 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ENV: CartPole-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "# Select Environment\n",
    "\n",
    "#discrete environment:\n",
    "envName = 'cartpole'\n",
    "# Environments to be tested on\n",
    "if envName == 'inverted pendulum':\n",
    "    env_name = 'InvertedPendulum-v1'\n",
    "elif envName =='cheetah':\n",
    "    env_name = 'HalfCheetah-v1' \n",
    "elif envName == 'cartpole':\n",
    "    env_name='CartPole-v0'\n",
    "print(\"Using ENV: \"+env_name)\n",
    "\n",
    "\n",
    "\n",
    "# Make the gym environment\n",
    "env = gym.make(env_name)\n",
    "visualize = True\n",
    "animate=visualize\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "max_path_length=500\n",
    "min_timesteps_per_batch = 2000\n",
    "\n",
    "# Set random seeds\n",
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Saving parameters\n",
    "logdir='./REINFORCE/'\n",
    "\n",
    "\n",
    "if visualize:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = gym.wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%animate_interval==0)\n",
    "env._max_episodes_steps = min_timesteps_per_batch\n",
    "\n",
    "# make variable types for automatic setting to GPU or CPU, depending on GPU availability\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "# Is this env continuous, or discrete?\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "# Get observation and action space dimensions\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "# Maximum length for episodes\n",
    "max_path_length = max_path_length or env.spec.max_episode_steps\n",
    "\n",
    "# Make network object (remember to pass in appropriate flags for the type of action space in use)\n",
    "# net = mlp(*args)\n",
    "net = mlp(obs_dim,act_dim,discrete).type(FloatTensor)\n",
    "# Make optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run REINFORCE\n",
    "\n",
    "Run REINFORCE for CartPole, InvertedPendulum, and HalfCheetah. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:05:54,638] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video000000.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:07:45,314] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video000100.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:07:48,174] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video000200.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:07:51,493] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video000300.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:07:54,441] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video000400.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:07:57,370] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video000500.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:08:01,480] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video000600.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:08:04,657] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video000700.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:08:07,771] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video000800.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:09:47,127] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video000900.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 24.829477331492672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:10:01,132] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video001000.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:10:03,955] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video001100.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:10:07,465] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video001200.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:10:11,675] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video001300.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:10:14,732] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video001400.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:10:19,438] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video001500.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 34.90757720075424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:12:12,456] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video001600.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:12:16,791] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video001700.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:12:20,267] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video001800.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:12:24,457] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video001900.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:12:29,493] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video002000.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:12:46,567] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video002100.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 38.57328730008678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:14:26,042] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video002200.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:14:32,022] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video002300.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:14:37,165] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video002400.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:14:42,129] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video002500.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:15:50,580] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video002600.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 46.20672716509851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:16:41,312] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video002700.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:16:46,802] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video002800.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:16:53,609] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video002900.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:17:54,153] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video003000.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 49.14166575098713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:18:52,844] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video003100.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:18:58,620] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video003200.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:19:05,280] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video003300.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 59.39971715980772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:21:00,397] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video003400.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:21:07,771] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video003500.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:21:16,831] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video003600.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 77.24735907375391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:23:10,999] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video003700.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:23:19,881] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video003800.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:23:29,946] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video003900.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 90.60875887744405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:25:36,556] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video004000.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 125.9066766938374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:27:38,227] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video004100.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:27:54,650] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video004200.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 141.56366297759556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:30:02,334] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video004300.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 150.5511148690984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:32:07,758] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video004400.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:34:21,563] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video004500.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 163.50725522640732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:34:42,693] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video004600.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 175.5526013153595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:36:57,769] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video004700.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 174.59232551852065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:39:13,867] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video004800.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 183.02124470759472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:41:32,350] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video004900.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 184.1434755384446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:43:46,291] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video005000.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 190.2075738828497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:46:06,472] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video005100.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 194.13691275889937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:48:20,757] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video005200.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:50:39,593] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video005300.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.29955309077576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 15:51:22,775] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video005400.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 193.88821748004594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:53:18,535] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video005500.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 192.645526290712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:55:33,210] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video005600.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 195.5966049215917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 15:57:52,425] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video005700.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.36648729703768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:00:11,954] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video005800.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 194.82432078977425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:02:30,335] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video005900.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 191.98953000731925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:04:38,333] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video006000.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 189.9070541447022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:06:48,434] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video006100.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 193.95698049070023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:09:07,826] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video006200.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 195.58182099524393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:11:24,185] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video006300.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 16:12:18,676] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video006400.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.35467302568503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:14:04,566] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video006500.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.55877002411387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:16:24,003] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video006600.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.73842690372962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:18:40,354] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video006700.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.1816450347211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:20:58,832] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video006800.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.32656552440156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:23:17,985] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video006900.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 198.39931602482574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:25:32,050] Starting new video recorder writing to /datasets/home/86/786/dorozco/REINFORCE/openaigym.video.1.202.video007000.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[2018-05-22 16:27:46,156] Finished writing results. You can upload them to the scoreboard via gym.upload('/datasets/home/86/786/dorozco/REINFORCE')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.0416113760162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Average reward')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_iter = 1000 \n",
    "min_timesteps_per_batch = 2000  # sets the batch size for updating network\n",
    "avg_reward = 0\n",
    "avg_rewards_reinforce = []\n",
    "step_list_reinforce = []\n",
    "total_steps = 0\n",
    "episodes = 0\n",
    "logging_interval = 10\n",
    "\n",
    "if envName =='cartpole':\n",
    "    term_condition = 199 # Pendulum\n",
    "elif envName == 'inverted pendulum':\n",
    "    term_condition = 500\n",
    "elif envName == 'cheetah':\n",
    "    term_condition = 1500\n",
    "    \n",
    "for itr in range(n_iter): # loop for number of optimization steps\n",
    "    paths = []\n",
    "    steps = 0\n",
    "    animate_this_episode = (itr % logging_interval == 0) and VISUALIZE\n",
    "    while True: # loop to get enough timesteps in this batch --> if episode ends this loop will restart till steps reaches limit\n",
    "        ob = env.reset()\n",
    "        ob = np.reshape(ob, [1, ob.size])\n",
    "        obs, acs, rews, log_odds = [], [], [], [] \n",
    "        \n",
    "        while True: # loop for episode inside batch\n",
    "            if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "            \n",
    "            # get parametrized policy distribution from net using current state ob\n",
    "            logit  = net(Variable(torch.from_numpy(ob).float().cuda()))\n",
    "            # sample action and get log-probability (log_odds) from distribution\n",
    "            action, log_odd = sample_action(logit, discrete)\n",
    "            #print(action)\n",
    "            action = action.cpu().detach().numpy()[0]\n",
    "            #print(action)\n",
    "            # step environment, record reward, next state\n",
    "            obs.append(ob)\n",
    "            ob,rewards,done,_= env.step(action)\n",
    "            ob = np.reshape(ob, [1, ob.size])\n",
    "            # append to obs, acs, rewards, log_odds\n",
    "            acs.append(action)\n",
    "            rews.append(rewards)\n",
    "            #print(log_odd)\n",
    "            log_odds.append(log_odd)\n",
    "            # if done, restart episode till min_timesteps_per_batch is reached\n",
    "                     \n",
    "            steps += 1\n",
    "            if done:\n",
    "                episodes = episodes + 1\n",
    "                break\n",
    "                \n",
    "        path = {\"observation\" : obs, \n",
    "                \"reward\" : np.array(rews), \n",
    "                \"action\" : (acs),\n",
    "                \"log_odds\" : log_odds}\n",
    "        \n",
    "        paths.append(path)\n",
    "        \n",
    "        if steps > min_timesteps_per_batch:\n",
    "            break \n",
    "        \n",
    "    update_policy(paths, net)  # use all complete episodes (a batch of timesteps) recorded in this itr to update net\n",
    "    \n",
    "    if itr == 0:\n",
    "        avg_reward = path['reward'].sum()\n",
    "    else:\n",
    "        avg_reward = avg_reward * 0.95 + 0.05 * path['reward'].sum()\n",
    "    \n",
    "    if avg_reward >= term_condition:\n",
    "        print(avg_reward)\n",
    "        break\n",
    "    \n",
    "    total_steps += steps\n",
    "    avg_rewards_reinforce.append(avg_reward)\n",
    "    step_list_reinforce.append(total_steps)\n",
    "    if itr % logging_interval == 0:\n",
    "        print('Average reward: {}'.format(avg_reward))\n",
    "   \n",
    "      \n",
    "env.close()\n",
    "\n",
    "plt.plot(avg_rewards_reinforce)\n",
    "plt.title('Training reward for {} using REINFORCE '.format(envName))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Average reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS (15% extra)\n",
    "\n",
    "Compare average returns for CartPole (discrete action space) when using REINFORCE and DQN. Since in REINFORCE we update the network after a set number of steps instead of after every episode, plot the average rewards as a function of steps rather than episodes for both DQN and REINFORCE. You will need to make minor edits to your DQN code from the previous assignment to record average returns as a function of time_steps.\n",
    "\n",
    "Similarly, compare REINFORCE with DDPG on InvertedPendulum and HalfCheetah using steps for the x-axis.\n",
    "\n",
    "You may use the example code provided below as a reference for the graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:27:48,113] Making new env: CartPole-v0\n",
      "[2018-05-22 16:27:48,167] Clearing 52 monitor files from previous run (because force=True was provided)\n",
      "[2018-05-22 16:27:48,239] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (fc1): Linear(in_features=4, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (fc3): Linear(in_features=20, out_features=2, bias=True)\n",
      "  (criterion): MSELoss()\n",
      ")\n",
      "QNetwork(\n",
      "  (fc1): Linear(in_features=4, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (fc3): Linear(in_features=20, out_features=2, bias=True)\n",
      "  (criterion): MSELoss()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 2.65775\n",
      "Average reward: 7.694594411640623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:27:51,779] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000020.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 19.75832340206416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:28:00,084] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000040.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:28:04,480] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000060.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 18.848353886106587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:28:08,373] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000080.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 20.59217410235478\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:28:12,779] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000100.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "[2018-05-22 16:28:16,659] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000120.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "[2018-05-22 16:28:20,280] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000140.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "Average reward: 33.602179736470305\n",
      "Average reward: 32.314416658978416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:28:23,185] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000160.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "[2018-05-22 16:28:27,152] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000180.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 37.25798284287882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 37.91074564352168\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:28:30,396] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000200.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 45.61982974835548\n",
      "Average reward: 47.09817653049627\n",
      "Average reward: 45.645296647692525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:28:33,887] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000220.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 43.56626372676686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "[2018-05-22 16:28:40,312] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000240.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 53.78411404802477\n",
      "250\n",
      "Average reward: 64.25539620912673\n",
      "Average reward: 61.62534143222541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:28:51,578] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000260.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 73.63899575945074\n",
      "Average reward: 83.4114836753063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:28:56,078] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000280.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 77.93285204072818\n",
      "Average reward: 91.20057639130887\n",
      "300\n",
      "Average reward: 100.08946918349844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:29:10,582] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000300.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "[2018-05-22 16:29:18,992] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000320.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 98.03007421954052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:29:28,191] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000340.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:29:39,815] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000360.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 98.2723440693358\n",
      "Average reward: 104.58079052257555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:29:50,999] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000380.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 92.01402042940369\n",
      "Average reward: 97.6978181320827\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:29:56,681] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000400.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 112.90741445173516\n",
      "Average reward: 121.0517884922786\n",
      "Average reward: 124.99919906766466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:30:11,293] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000420.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 128.36697263560703\n",
      "Average reward: 131.94862400382667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 135.35119280363534\n",
      "Average reward: 138.58363316345356\n",
      "Average reward: 141.65445150528086\n",
      "Average reward: 144.5717289300168\n",
      "Average reward: 147.34314248351595\n",
      "Average reward: 149.97598535934014\n",
      "Average reward: 152.47718609137314\n",
      "Average reward: 154.85332678680447\n",
      "Average reward: 157.11066044746426\n",
      "Average reward: 159.25512742509105\n",
      "Average reward: 161.2923710538365\n",
      "Average reward: 163.22775250114466\n",
      "Average reward: 166.00554663228306\n",
      "Average reward: 156.77263054385367\n",
      "Average reward: 158.08979906582792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:30:26,866] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000440.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 153.1985436569097\n",
      "Average reward: 155.5386164740642\n",
      "Average reward: 156.6560780352543\n",
      "450\n",
      "Average reward: 159.69461042681698\n",
      "Average reward: 161.70987990547613\n",
      "Average reward: 164.82566661469218\n",
      "Average reward: 164.58438328395758\n",
      "Average reward: 163.04990591377168\n",
      "Average reward: 164.8974106180831\n",
      "Average reward: 166.65254008717892\n",
      "Average reward: 168.31991308281997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:30:40,869] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000460.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 170.696221557245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 172.1283399554136\n",
      "Average reward: 171.1588216985186\n",
      "Average reward: 172.60088061359266\n",
      "Average reward: 173.97083658291302\n",
      "Average reward: 175.27229475376737\n",
      "Average reward: 176.50868001607898\n",
      "Average reward: 177.68324601527502\n",
      "Average reward: 178.10162952878568\n",
      "Average reward: 178.52672064972907\n",
      "Average reward: 179.62286538638045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:30:55,016] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000480.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 172.00576282350528\n",
      "Average reward: 173.40547468233\n",
      "Average reward: 174.7352009482135\n",
      "Average reward: 176.34834291297454\n",
      "Average reward: 177.5309257673258\n",
      "Average reward: 178.29666050501154\n",
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:31:10,183] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000500.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 178.12309504400267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 178.5455963385489\n",
      "Average reward: 179.61831652162144\n",
      "Average reward: 173.2735515377719\n",
      "Average reward: 175.35688026283913\n",
      "Average reward: 176.58903624969716\n",
      "Average reward: 178.49160521535165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:31:22,978] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000520.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 170.40010286941467\n",
      "Average reward: 172.9060928396467\n",
      "Average reward: 175.35774878778116\n",
      "Average reward: 170.5898613483921\n",
      "Average reward: 172.41234986692385\n",
      "Average reward: 175.72587404379613\n",
      "Average reward: 177.04760132452597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:31:35,785] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000540.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n",
      "Average reward: 168.71925971815045\n",
      "Average reward: 170.05913189563074\n",
      "Average reward: 172.43053759570543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:31:49,059] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000560.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 173.80901071592015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 173.9401321711179\n",
      "Average reward: 173.9159692844339\n",
      "Average reward: 174.52970416524153\n",
      "Average reward: 176.53805800913045\n",
      "Average reward: 178.20809735324022\n",
      "Average reward: 179.2976924855782\n",
      "Average reward: 180.3328078612993\n",
      "Average reward: 181.31616746823434\n",
      "Average reward: 181.80784114008148\n",
      "Average reward: 181.7765766289235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:32:01,884] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000580.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 180.19094238722326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 181.1813952678621\n",
      "Average reward: 182.1612092292455\n",
      "Average reward: 178.55230687853918\n",
      "Average reward: 179.62469153461223\n",
      "Average reward: 181.5637841099875\n",
      "Average reward: 178.18097443123548\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:32:14,793] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000600.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 177.35038054580846\n",
      "Average reward: 178.27621844259212\n",
      "Average reward: 179.3624075204625\n",
      "Average reward: 178.8851519404637\n",
      "Average reward: 179.37458053770726\n",
      "Average reward: 180.4058515108219\n",
      "Average reward: 181.84128098851673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:32:30,302] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000620.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 181.42675609213632\n",
      "Average reward: 182.3554182875295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 183.237647373153\n",
      "Average reward: 184.07576500449534\n",
      "Average reward: 184.87197675427058\n",
      "Average reward: 185.87195902072918\n",
      "Average reward: 186.5783610696927\n",
      "Average reward: 187.24944301620806\n",
      "Average reward: 188.11262232212775\n",
      "Average reward: 188.70699120602134\n",
      "Average reward: 188.90555956343428\n",
      "Average reward: 189.46028158526255\n",
      "Average reward: 185.02540413069946\n",
      "Average reward: 185.7741339241645\n",
      "Average reward: 186.48542722795625\n",
      "Average reward: 187.16115586655843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:32:46,963] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000640.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 187.8030980732305\n",
      "Average reward: 188.41294316956896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 186.97567215000916\n",
      "Average reward: 187.62688854250868\n",
      "Average reward: 188.24554411538324\n",
      "Average reward: 187.7916483859267\n",
      "650\n",
      "Average reward: 188.40206596663035\n",
      "Average reward: 189.24786453488386\n",
      "Average reward: 184.64438785559602\n",
      "Average reward: 185.41216846281623\n",
      "Average reward: 184.6969820376916\n",
      "Average reward: 185.46213293580703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:33:02,305] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000660.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 186.18902628901668\n",
      "Average reward: 186.87957497456583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 187.53559622583754\n",
      "Average reward: 188.15881641454564\n",
      "Average reward: 188.75087559381836\n",
      "Average reward: 189.31333181412745\n",
      "Average reward: 189.2676428641375\n",
      "Average reward: 189.4115476848841\n",
      "Average reward: 189.9409703006399\n",
      "Average reward: 190.4439217856079\n",
      "Average reward: 189.8556394115111\n",
      "Average reward: 190.36285744093556\n",
      "Average reward: 189.7067298984221\n",
      "Average reward: 190.221393403501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:33:19,232] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000680.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 190.71032373332594\n",
      "Average reward: 191.17480754665962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 191.61606716932664\n",
      "Average reward: 192.0352638108603\n",
      "Average reward: 190.05285930983632\n",
      "Average reward: 190.5502163443445\n",
      "Average reward: 189.85657025077091\n",
      "Average reward: 190.32305465132072\n",
      "Average reward: 190.80690191875468\n",
      "Average reward: 191.26655682281694\n",
      "Average reward: 192.07056753259226\n",
      "Average reward: 192.3686871981645\n",
      "Average reward: 192.21024019634345\n",
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:33:34,682] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000700.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 192.59972818652628\n",
      "Average reward: 192.96974177719994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 193.32125468833993\n",
      "Average reward: 193.65519195392292\n",
      "Average reward: 193.97243235622676\n",
      "Average reward: 194.27381073841542\n",
      "Average reward: 194.56012020149464\n",
      "Average reward: 194.23550848184888\n",
      "Average reward: 194.52373305775643\n",
      "Average reward: 194.7975464048686\n",
      "Average reward: 195.05766908462516\n",
      "Average reward: 194.51310307985892\n",
      "Average reward: 194.78744792586596\n",
      "Average reward: 195.04807552957266\n",
      "Average reward: 195.295671753094\n",
      "Average reward: 195.5308881654393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:33:49,778] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000720.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 195.63412656930896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 195.85242024084351\n",
      "Average reward: 196.05979922880132\n",
      "Average reward: 195.1114688039932\n",
      "Average reward: 195.35589536379354\n",
      "Average reward: 195.58810059560386\n",
      "Average reward: 195.80869556582365\n",
      "Average reward: 193.50305009271062\n",
      "Average reward: 193.82789758807507\n",
      "Average reward: 194.13650270867132\n",
      "Average reward: 194.42967757323774\n",
      "Average reward: 193.70819369457584\n",
      "Average reward: 194.02278400984704\n",
      "Average reward: 194.27306256888696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:34:05,683] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000740.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 194.5594094404426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 192.90486701999941\n",
      "Average reward: 193.25962366899944\n",
      "Average reward: 192.44209484320834\n",
      "Average reward: 192.81999010104792\n",
      "Average reward: 193.17899059599551\n",
      "750\n",
      "Average reward: 192.63658706224163\n",
      "Average reward: 191.97701982367306\n",
      "Average reward: 192.3781688324894\n",
      "Average reward: 192.7592603908649\n",
      "Average reward: 193.12129737132165\n",
      "Average reward: 193.46523250275555\n",
      "Average reward: 193.79197087761776\n",
      "Average reward: 194.10237233373687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:34:20,078] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000760.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 192.2740214796376\n",
      "Average reward: 192.6603204056557\n",
      "Average reward: 193.02730438537293\n",
      "Average reward: 192.8876208425386\n",
      "Average reward: 193.24323980041166\n",
      "Average reward: 193.58107781039106\n",
      "Average reward: 192.92442272387794\n",
      "Average reward: 193.27820158768404\n",
      "Average reward: 193.2607730862406\n",
      "Average reward: 193.59773443192856\n",
      "Average reward: 193.91784771033213\n",
      "Average reward: 194.2219553248155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:34:34,479] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000780.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 194.5108575585747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 193.24104894661366\n",
      "Average reward: 193.57899649928297\n",
      "Average reward: 193.90004667431882\n",
      "Average reward: 194.20504434060285\n",
      "Average reward: 194.4947921235727\n",
      "Average reward: 193.98654989152433\n",
      "Average reward: 194.2872223969481\n",
      "Average reward: 194.57286127710069\n",
      "Average reward: 194.84421821324565\n",
      "Average reward: 195.10200730258336\n",
      "Average reward: 195.34690693745418\n",
      "Average reward: 195.18308351105236\n",
      "Average reward: 195.41523286872473\n",
      "Average reward: 195.64447122528847\n",
      "Average reward: 195.86224766402404\n",
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:34:52,681] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000800.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.06913528082282\n",
      "Average reward: 196.26567851678166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.45239459094256\n",
      "Average reward: 195.70578611832562\n",
      "Average reward: 195.92049681240934\n",
      "Average reward: 196.12447197178886\n",
      "Average reward: 195.36233595453945\n",
      "Average reward: 195.59421915681247\n",
      "Average reward: 195.81450819897185\n",
      "Average reward: 196.02378278902324\n",
      "Average reward: 196.22259364957208\n",
      "Average reward: 194.40589076873877\n",
      "Average reward: 194.68559623030183\n",
      "Average reward: 194.95131641878672\n",
      "Average reward: 195.20375059784737\n",
      "Average reward: 195.44356306795498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:35:06,513] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000820.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 193.29567488538788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 193.6643465840625\n",
      "Average reward: 193.98112925485938\n",
      "Average reward: 194.2820727921164\n",
      "Average reward: 194.56796915251056\n",
      "Average reward: 194.83957069488503\n",
      "Average reward: 195.09759216014078\n",
      "Average reward: 195.34271255213372\n",
      "Average reward: 195.575576924527\n",
      "Average reward: 193.9169581743856\n",
      "Average reward: 194.2211102656663\n",
      "Average reward: 194.51005475238298\n",
      "Average reward: 194.0478244140256\n",
      "Average reward: 194.3454331933243\n",
      "Average reward: 194.62816153365807\n",
      "Average reward: 194.89675345697515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:35:21,281] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000840.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 195.15191578412637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 194.0454987954153\n",
      "Average reward: 194.34322385564454\n",
      "Average reward: 194.6260626628623\n",
      "Average reward: 193.9150215532332\n",
      "Average reward: 194.21927047557153\n",
      "Average reward: 194.50830695179295\n",
      "850\n",
      "Average reward: 194.37874702399313\n",
      "Average reward: 194.65980967279347\n",
      "Average reward: 194.92681918915378\n",
      "Average reward: 195.18047822969606\n",
      "Average reward: 195.42145431821126\n",
      "Average reward: 195.65038160230068\n",
      "Average reward: 195.86786252218565\n",
      "Average reward: 196.07446939607635\n",
      "Average reward: 196.27074592627253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:35:35,828] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000860.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 195.58934819846095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 195.829386749111\n",
      "Average reward: 195.66602154107267\n",
      "Average reward: 195.88272046401903\n",
      "Average reward: 196.04665521877715\n",
      "Average reward: 196.24432245783828\n",
      "Average reward: 196.43210633494635\n",
      "Average reward: 196.61050101819902\n",
      "Average reward: 196.77997596728906\n",
      "Average reward: 196.9409771689246\n",
      "Average reward: 197.09392831047836\n",
      "Average reward: 197.23923189495443\n",
      "Average reward: 197.3772703002067\n",
      "Average reward: 197.50840678519637\n",
      "Average reward: 197.63298644593655\n",
      "Average reward: 197.75133712363973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:35:51,398] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000880.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.75867766638058\n",
      "Average reward: 196.92074378306154\n",
      "Average reward: 197.07470659390845\n",
      "Average reward: 197.220971264213\n",
      "Average reward: 197.35992270100235\n",
      "Average reward: 197.4919265659522\n",
      "Average reward: 197.61733023765458\n",
      "Average reward: 197.73646372577184\n",
      "Average reward: 197.84964053948323\n",
      "Average reward: 197.95715851250907\n",
      "Average reward: 198.0593005868836\n",
      "Average reward: 198.15633555753942\n",
      "Average reward: 198.24851877966245\n",
      "Average reward: 196.89928819864537\n",
      "Average reward: 197.05432378871308\n",
      "900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:36:05,904] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000900.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 194.32257585834785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 193.35612471215893\n",
      "Average reward: 192.4839025527234\n",
      "Average reward: 192.85970742508724\n",
      "Average reward: 193.21672205383285\n",
      "Average reward: 193.5558859511412\n",
      "Average reward: 193.87809165358414\n",
      "Average reward: 194.18418707090493\n",
      "Average reward: 194.47497771735968\n",
      "Average reward: 194.7512288314917\n",
      "Average reward: 195.0136673899171\n",
      "Average reward: 195.45233481940014\n",
      "Average reward: 195.6797180784301\n",
      "Average reward: 195.8957321745086\n",
      "Average reward: 196.10094556578315\n",
      "Average reward: 196.29589828749397\n",
      "Average reward: 196.48110337311925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:36:20,588] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000920.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.65704820446328\n",
      "Average reward: 196.8241957942401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.94383670430167\n",
      "Average reward: 197.0966448690866\n",
      "Average reward: 197.24181262563226\n",
      "Average reward: 197.37972199435063\n",
      "Average reward: 197.5107358946331\n",
      "Average reward: 197.63519909990143\n",
      "Average reward: 196.77326718766102\n",
      "Average reward: 196.93460382827797\n",
      "Average reward: 197.08787363686406\n",
      "Average reward: 197.23347995502084\n",
      "Average reward: 197.3718059572698\n",
      "Average reward: 197.5032156594063\n",
      "Average reward: 197.62805487643598\n",
      "Average reward: 197.74665213261417\n",
      "Average reward: 197.85931952598347\n",
      "Average reward: 197.96635354968427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:36:37,784] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000940.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 198.06803587220006\n",
      "Average reward: 198.16463407859004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 198.25640237466052\n",
      "Average reward: 197.95883298597454\n",
      "Average reward: 195.45695443134989\n",
      "950\n",
      "Average reward: 195.23490137429326\n",
      "Average reward: 195.47315630557858\n",
      "Average reward: 195.69949849029965\n",
      "Average reward: 195.91452356578466\n",
      "Average reward: 196.11879738749542\n",
      "Average reward: 196.31285751812064\n",
      "Average reward: 196.4972146422146\n",
      "Average reward: 196.67235391010385\n",
      "Average reward: 196.83873621459864\n",
      "Average reward: 196.9967994038687\n",
      "Average reward: 197.14695943367525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:36:56,823] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000960.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.28961146199148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.4251308888919\n",
      "Average reward: 197.5538743444473\n",
      "Average reward: 196.10274661526694\n",
      "Average reward: 196.19772882027837\n",
      "Average reward: 196.38784237926444\n",
      "Average reward: 196.5684502603012\n",
      "Average reward: 196.74002774728615\n",
      "Average reward: 196.90302635992182\n",
      "Average reward: 197.0578750419257\n",
      "Average reward: 197.20498128982942\n",
      "Average reward: 197.34473222533794\n",
      "Average reward: 197.47749561407102\n",
      "Average reward: 197.60362083336744\n",
      "Average reward: 197.72343979169906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:37:11,793] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video000980.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.66040441200838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.77738419140795\n",
      "Average reward: 197.88851498183755\n",
      "Average reward: 197.99408923274567\n",
      "Average reward: 198.09438477110837\n",
      "Average reward: 198.18966553255294\n",
      "Average reward: 198.28018225592527\n",
      "Average reward: 197.07036448597256\n",
      "Average reward: 197.21684626167394\n",
      "Average reward: 197.3932037511607\n",
      "Average reward: 197.52354356360266\n",
      "Average reward: 197.28999806615138\n",
      "Average reward: 197.4592232547016\n",
      "Average reward: 197.58626209196652\n",
      "Average reward: 197.70694898736818\n",
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:37:26,776] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.82160153799975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.36899538804477\n",
      "Average reward: 197.50054561864252\n",
      "Average reward: 197.6255183377104\n",
      "Average reward: 197.0495302997836\n",
      "Average reward: 196.4822010955547\n",
      "Average reward: 196.65809104077695\n",
      "Average reward: 196.27142716430117\n",
      "Average reward: 196.45785580608612\n",
      "Average reward: 195.76130412174305\n",
      "Average reward: 195.97323891565588\n",
      "Average reward: 196.17457696987307\n",
      "Average reward: 195.95067792954487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:37:45,455] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001020.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.1531440330676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.3454868314142\n",
      "Average reward: 196.5593018653513\n",
      "Average reward: 195.5647699334795\n",
      "Average reward: 194.23970486496523\n",
      "Average reward: 194.52771962171695\n",
      "Average reward: 194.8013336406311\n",
      "Average reward: 195.06126695859953\n",
      "Average reward: 194.87779343013608\n",
      "Average reward: 195.13390375862926\n",
      "Average reward: 194.8958481421629\n",
      "Average reward: 195.15105573505474\n",
      "Average reward: 195.393502948302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:38:01,574] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001040.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 194.41763641084253\n",
      "Average reward: 194.6967545903004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 194.9288210177461\n",
      "Average reward: 195.18237996685878\n",
      "Average reward: 195.42326096851582\n",
      "Average reward: 195.65209792009003\n",
      "Average reward: 195.86949302408553\n",
      "Average reward: 196.07601837288124\n",
      "1050\n",
      "Average reward: 194.8911065815253\n",
      "Average reward: 195.14655125244903\n",
      "Average reward: 195.38922368982657\n",
      "Average reward: 195.61976250533522\n",
      "Average reward: 194.57433566106502\n",
      "Average reward: 194.84561887801175\n",
      "Average reward: 195.10333793411115\n",
      "Average reward: 195.3481710374056\n",
      "Average reward: 195.5807624855353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:38:16,733] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001060.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 195.80172436125852\n",
      "Average reward: 196.0116381431956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.2110562360358\n",
      "Average reward: 196.400503424234\n",
      "Average reward: 196.58047825302228\n",
      "Average reward: 196.75145434037117\n",
      "Average reward: 196.9138816233526\n",
      "Average reward: 197.06818754218497\n",
      "Average reward: 195.80008729398082\n",
      "Average reward: 196.01008292928177\n",
      "Average reward: 195.20957878281766\n",
      "Average reward: 195.44909984367678\n",
      "Average reward: 195.67664485149294\n",
      "Average reward: 195.8928126089183\n",
      "Average reward: 196.09817197847238\n",
      "Average reward: 196.29326337954876\n",
      "Average reward: 196.4786002105713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:38:30,922] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001080.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.22321485553854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.4120541127616\n",
      "Average reward: 196.59145140712351\n",
      "Average reward: 195.73628489492896\n",
      "Average reward: 195.9494706501825\n",
      "Average reward: 187.18867739870018\n",
      "Average reward: 187.82924352876518\n",
      "Average reward: 188.4377813523269\n",
      "Average reward: 189.01589228471056\n",
      "Average reward: 189.565097670475\n",
      "Average reward: 190.08684278695125\n",
      "Average reward: 190.5825006476037\n",
      "Average reward: 190.9307068344623\n",
      "Average reward: 191.3841714927392\n",
      "Average reward: 191.81496291810222\n",
      "1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:38:46,304] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001100.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 192.22421477219712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 191.49261114031248\n",
      "Average reward: 191.91798058329684\n",
      "Average reward: 192.32208155413198\n",
      "Average reward: 192.70597747642537\n",
      "Average reward: 193.03714467247386\n",
      "Average reward: 192.67102306690765\n",
      "Average reward: 193.2460684019899\n",
      "Average reward: 193.5837649818904\n",
      "Average reward: 193.11684789615606\n",
      "Average reward: 193.46100550134824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:39:01,985] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001120.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 191.79537959171842\n",
      "Average reward: 192.2056106121325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 192.59533008152584\n",
      "Average reward: 192.96556357744953\n",
      "Average reward: 193.31728539857704\n",
      "Average reward: 193.65142112864817\n",
      "Average reward: 193.3266371901747\n",
      "Average reward: 193.66030533066595\n",
      "Average reward: 193.97729006413263\n",
      "Average reward: 194.278425560926\n",
      "Average reward: 194.56450428287968\n",
      "Average reward: 193.7644651152989\n",
      "Average reward: 193.08992976655722\n",
      "Average reward: 193.43543327822934\n",
      "Average reward: 193.76366161431787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:39:17,359] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001140.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 192.51920460692187\n",
      "Average reward: 192.89324437657578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 193.248582157747\n",
      "Average reward: 193.58615304985963\n",
      "Average reward: 193.90684539736665\n",
      "Average reward: 193.26592797112338\n",
      "Average reward: 193.6026315725672\n",
      "Average reward: 193.92249999393883\n",
      "Average reward: 194.22637499424187\n",
      "1150\n",
      "Average reward: 194.51505624452977\n",
      "Average reward: 194.78930343230329\n",
      "Average reward: 195.04983826068812\n",
      "Average reward: 195.2973463476537\n",
      "Average reward: 194.28335507875744\n",
      "Average reward: 194.56918732481955\n",
      "Average reward: 194.84072795857855\n",
      "Average reward: 193.77625698261713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:39:32,483] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001160.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 193.33807192681195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 193.67116833047135\n",
      "Average reward: 193.67072941825037\n",
      "Average reward: 193.98719294733786\n",
      "Average reward: 194.28783329997097\n",
      "Average reward: 194.57344163497243\n",
      "Average reward: 194.84476955322378\n",
      "Average reward: 195.10253107556258\n",
      "Average reward: 195.15253429569523\n",
      "Average reward: 195.39490758091046\n",
      "Average reward: 195.62516220186492\n",
      "Average reward: 195.84390409177166\n",
      "Average reward: 196.05170888718308\n",
      "Average reward: 196.24912344282393\n",
      "Average reward: 196.43666727068273\n",
      "Average reward: 196.11909221179116\n",
      "Average reward: 196.3131376012016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:39:49,509] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001180.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.4974807211415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.67260668508442\n",
      "Average reward: 196.71202753328868\n",
      "Average reward: 196.87642615662423\n",
      "Average reward: 197.032604848793\n",
      "Average reward: 197.22692587603566\n",
      "Average reward: 196.30980060312217\n",
      "Average reward: 196.49431057296604\n",
      "Average reward: 196.66959504431773\n",
      "Average reward: 195.64834405112188\n",
      "Average reward: 195.86592684856578\n",
      "Average reward: 196.07263050613747\n",
      "Average reward: 196.2689989808306\n",
      "Average reward: 196.45554903178905\n",
      "1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:40:04,176] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001200.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.63277158019957\n",
      "Average reward: 196.80113300118958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.9610763511301\n",
      "Average reward: 194.15500283655015\n",
      "Average reward: 194.6773900599865\n",
      "Average reward: 194.94352055698715\n",
      "Average reward: 195.19634452913778\n",
      "Average reward: 195.43652730268087\n",
      "Average reward: 194.88396589066946\n",
      "Average reward: 194.05277921632916\n",
      "Average reward: 194.35014025551268\n",
      "Average reward: 194.63263324273703\n",
      "Average reward: 194.90100158060017\n",
      "Average reward: 195.15595150157014\n",
      "Average reward: 195.39815392649163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:40:19,950] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001220.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 195.62824623016704\n",
      "Average reward: 195.8468339186587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 195.68176761158946\n",
      "Average reward: 195.29529526945947\n",
      "Average reward: 195.5305305059865\n",
      "Average reward: 195.4913037816528\n",
      "Average reward: 195.69340166294162\n",
      "Average reward: 195.90873157979453\n",
      "Average reward: 196.1132950008048\n",
      "Average reward: 196.30763025076456\n",
      "Average reward: 196.49224873822632\n",
      "Average reward: 196.66763630131499\n",
      "Average reward: 196.83425448624922\n",
      "Average reward: 196.99254176193676\n",
      "Average reward: 197.14291467383993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:40:36,783] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001240.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 195.3489486450593\n",
      "Average reward: 195.6370548445577\n",
      "Average reward: 195.8552021023298\n",
      "Average reward: 196.0624419972133\n",
      "Average reward: 196.25931989735264\n",
      "Average reward: 196.446353902485\n",
      "1250\n",
      "Average reward: 196.62403620736072\n",
      "Average reward: 196.79283439699267\n",
      "Average reward: 196.95319267714302\n",
      "Average reward: 197.10553304328587\n",
      "Average reward: 197.25025639112158\n",
      "Average reward: 196.59031357333782\n",
      "Average reward: 196.76079789467093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:40:52,209] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001260.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.22162009994048\n",
      "Average reward: 196.41053909494346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.59001214019628\n",
      "Average reward: 196.76051153318645\n",
      "Average reward: 196.9224859565271\n",
      "Average reward: 196.60504357576568\n",
      "Average reward: 196.3660518271285\n",
      "Average reward: 196.54774923577207\n",
      "Average reward: 196.72036177398346\n",
      "Average reward: 196.88434368528428\n",
      "Average reward: 197.04012650102007\n",
      "Average reward: 197.18812017596906\n",
      "Average reward: 197.3287141671706\n",
      "Average reward: 197.46227845881205\n",
      "Average reward: 197.58916453587145\n",
      "Average reward: 197.70970630907786\n",
      "Average reward: 197.82422099362395\n",
      "Average reward: 197.93300994394275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:41:09,156] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001280.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.65954147440831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.7765644006879\n",
      "Average reward: 197.8877361806535\n",
      "Average reward: 197.9933493716208\n",
      "Average reward: 198.09368190303977\n",
      "Average reward: 198.18899780788777\n",
      "Average reward: 198.27954791749337\n",
      "Average reward: 198.3655705216187\n",
      "Average reward: 198.44729199553777\n",
      "Average reward: 198.52492739576087\n",
      "Average reward: 198.5986810259728\n",
      "Average reward: 198.66874697467415\n",
      "Average reward: 198.73530962594043\n",
      "Average reward: 198.0036169374112\n",
      "Average reward: 196.9157642860136\n",
      "Average reward: 197.06997607171292\n",
      "Average reward: 197.21647726812725\n",
      "1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:41:24,301] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001300.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.2028707344848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.34272719776055\n",
      "Average reward: 197.47559083787252\n",
      "Average reward: 197.60181129597888\n",
      "Average reward: 197.7881346946209\n",
      "Average reward: 197.89872795988984\n",
      "Average reward: 198.00379156189533\n",
      "Average reward: 196.7259218846105\n",
      "Average reward: 196.88962579037997\n",
      "Average reward: 196.1478872758179\n",
      "Average reward: 196.340492912027\n",
      "Average reward: 196.52346826642565\n",
      "Average reward: 196.69729485310435\n",
      "Average reward: 196.86243011044914\n",
      "Average reward: 197.01930860492666\n",
      "Average reward: 197.16834317468033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:41:41,709] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001320.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.3099260159463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.47720822939152\n",
      "Average reward: 197.60334781792193\n",
      "Average reward: 197.72318042702582\n",
      "Average reward: 197.83702140567453\n",
      "Average reward: 197.9451703353908\n",
      "Average reward: 198.04791181862126\n",
      "Average reward: 197.04857839549038\n",
      "Average reward: 197.19614947571586\n",
      "Average reward: 197.33634200193006\n",
      "Average reward: 197.46952490183355\n",
      "Average reward: 197.59604865674186\n",
      "Average reward: 197.71624622390476\n",
      "Average reward: 197.8304339127095\n",
      "Average reward: 197.37696660622032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:41:56,688] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001340.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.96771236211382\n",
      "Average reward: 197.1193267440081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.2633604068077\n",
      "Average reward: 197.4001923864673\n",
      "Average reward: 197.5301827671439\n",
      "Average reward: 197.65367362878672\n",
      "Average reward: 197.26494044998\n",
      "Average reward: 197.401693427481\n",
      "Average reward: 197.53160875610695\n",
      "1350\n",
      "Average reward: 197.6550283183016\n",
      "Average reward: 197.7722769023865\n",
      "Average reward: 197.88366305726717\n",
      "Average reward: 197.9894799044038\n",
      "Average reward: 198.0900059091836\n",
      "Average reward: 198.03873033303816\n",
      "Average reward: 198.13679381638624\n",
      "Average reward: 198.22995412556693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:42:12,649] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001360.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 198.11753359832412\n",
      "Average reward: 198.2116569184079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 198.3010740724875\n",
      "Average reward: 197.75421935041996\n",
      "Average reward: 197.86650838289896\n",
      "Average reward: 197.973182963754\n",
      "Average reward: 198.0745238155663\n",
      "Average reward: 198.17079762478798\n",
      "Average reward: 197.77914485637112\n",
      "Average reward: 197.89018761355254\n",
      "Average reward: 198.00089432123116\n",
      "Average reward: 197.9583071249111\n",
      "Average reward: 198.06039176866554\n",
      "Average reward: 196.87200357122063\n",
      "Average reward: 197.02840339265958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:42:27,508] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001380.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.17698322302658\n",
      "Average reward: 197.31813406187524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.4396159908424\n",
      "Average reward: 196.61763519130028\n",
      "Average reward: 196.78675343173526\n",
      "Average reward: 196.9474157601485\n",
      "Average reward: 197.10004497214106\n",
      "Average reward: 197.245042723534\n",
      "Average reward: 197.38279058735728\n",
      "Average reward: 197.5136510579894\n",
      "Average reward: 197.6379685050899\n",
      "Average reward: 197.7560700798354\n",
      "Average reward: 197.86826657584362\n",
      "Average reward: 197.97485324705144\n",
      "Average reward: 198.07611058469885\n",
      "Average reward: 198.1723050554639\n",
      "Average reward: 198.2636898026907\n",
      "Average reward: 198.35050531255615\n",
      "Average reward: 198.43298004692832\n",
      "1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:42:43,004] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001400.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.69947626773512\n",
      "Average reward: 195.7862773316309\n",
      "Average reward: 195.99696346504936\n",
      "Average reward: 196.19711529179688\n",
      "Average reward: 196.38725952720702\n",
      "Average reward: 196.56789655084665\n",
      "Average reward: 196.7395017233043\n",
      "Average reward: 196.9025266371391\n",
      "Average reward: 197.05740030528213\n",
      "Average reward: 196.20453029001803\n",
      "Average reward: 196.39430377551713\n",
      "Average reward: 196.57458858674127\n",
      "Average reward: 196.7458591574042\n",
      "Average reward: 196.90856619953396\n",
      "Average reward: 197.06313788955725\n",
      "Average reward: 197.20998099507938\n",
      "Average reward: 197.3494819453254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:43:00,094] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001420.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.48200784805914\n",
      "Average reward: 197.60790745565617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.72751208287335\n",
      "Average reward: 197.84113647872968\n",
      "Average reward: 197.9490796547932\n",
      "Average reward: 198.05162567205352\n",
      "Average reward: 198.14904438845085\n",
      "Average reward: 198.2415921690283\n",
      "Average reward: 198.3295125605769\n",
      "Average reward: 198.41303693254804\n",
      "Average reward: 198.49238508592063\n",
      "Average reward: 198.5677658316246\n",
      "Average reward: 198.63937754004337\n",
      "Average reward: 197.82643631839463\n",
      "Average reward: 197.93511450247487\n",
      "Average reward: 198.03835877735114\n",
      "Average reward: 195.79588785673138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:43:17,125] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001440.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.0060934638948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.20578879070007\n",
      "Average reward: 195.3882243836068\n",
      "Average reward: 195.61881316442646\n",
      "Average reward: 195.47597888089484\n",
      "Average reward: 195.7021799368501\n",
      "Average reward: 195.91707094000756\n",
      "1450\n",
      "Average reward: 196.12121739300719\n",
      "Average reward: 196.31515652335682\n",
      "Average reward: 196.49939869718898\n",
      "Average reward: 196.6744287623295\n",
      "Average reward: 196.840707324213\n",
      "Average reward: 196.99867195800235\n",
      "Average reward: 197.14873836010221\n",
      "Average reward: 197.23673636999226\n",
      "Average reward: 197.37489955149263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:43:31,288] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001460.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.39584684522208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.57605450296097\n",
      "Average reward: 196.74725177781292\n",
      "Average reward: 196.90988918892228\n",
      "Average reward: 197.06439472947616\n",
      "Average reward: 197.21117499300234\n",
      "Average reward: 197.35061624335222\n",
      "Average reward: 196.51643115962537\n",
      "Average reward: 196.6906096016441\n",
      "Average reward: 196.8560791215619\n",
      "Average reward: 197.01327516548378\n",
      "Average reward: 197.16261140720957\n",
      "Average reward: 197.30448083684908\n",
      "Average reward: 197.43925679500663\n",
      "Average reward: 197.56729395525628\n",
      "Average reward: 197.68892925749344\n",
      "Average reward: 197.43925865488782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:43:47,135] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001480.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.56729572214343\n",
      "Average reward: 197.68893093603626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.80448438923443\n",
      "Average reward: 197.9142601697727\n",
      "Average reward: 196.78761980321983\n",
      "Average reward: 196.94823881305882\n",
      "Average reward: 197.10082687240586\n",
      "Average reward: 197.24578552878555\n",
      "Average reward: 196.89285536774247\n",
      "Average reward: 197.04821259935534\n",
      "Average reward: 197.19580196938756\n",
      "Average reward: 197.33601187091818\n",
      "Average reward: 197.46921127737227\n",
      "Average reward: 197.59575071350363\n",
      "Average reward: 197.71596317782846\n",
      "Average reward: 197.83016501893704\n",
      "Average reward: 197.93865676799018\n",
      "1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:44:02,259] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001500.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 198.04172392959066\n",
      "Average reward: 198.13963773311113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 198.23265584645557\n",
      "Average reward: 198.32102305413278\n",
      "Average reward: 198.40497190142614\n",
      "Average reward: 198.48472330635482\n",
      "Average reward: 198.56048714103707\n",
      "Average reward: 198.6324627839852\n",
      "Average reward: 198.70083964478593\n",
      "Average reward: 198.76579766254662\n",
      "Average reward: 198.82750777941928\n",
      "Average reward: 198.8861323904483\n",
      "Average reward: 197.94182577092587\n",
      "Average reward: 198.04473448237957\n",
      "Average reward: 198.1424977582606\n",
      "Average reward: 198.23537287034756\n",
      "Average reward: 198.32360422683018\n",
      "Average reward: 198.2495528147142\n",
      "Average reward: 198.3370751739785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:44:16,382] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001520.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.42022141527957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.4342498272898\n",
      "Average reward: 197.5625373359253\n",
      "Average reward: 197.68441046912903\n",
      "Average reward: 197.80018994567257\n",
      "Average reward: 196.30467142596947\n",
      "Average reward: 196.489437854671\n",
      "Average reward: 196.45171766384055\n",
      "Average reward: 196.62913178064852\n",
      "Average reward: 196.7976751916161\n",
      "Average reward: 196.9577914320353\n",
      "Average reward: 197.10990186043352\n",
      "Average reward: 197.25440676741184\n",
      "Average reward: 196.52460210758917\n",
      "Average reward: 196.6983720022097\n",
      "Average reward: 196.86345340209922\n",
      "Average reward: 197.02028073199426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:44:30,983] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001540.mp4\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.8358033606248\n",
      "Average reward: 196.99401319259354\n",
      "Average reward: 197.14431253296385\n",
      "Average reward: 197.28709690631564\n",
      "Average reward: 197.40910495794984\n",
      "Average reward: 197.53864971005234\n",
      "Average reward: 197.6617172245497\n",
      "1550\n",
      "Average reward: 197.7786313633222\n",
      "Average reward: 197.88969979515608\n",
      "Average reward: 197.90545406512834\n",
      "Average reward: 198.0101813618719\n",
      "Average reward: 198.1096722937783\n",
      "Average reward: 198.20418867908936\n",
      "Average reward: 198.2939792451349\n",
      "Average reward: 198.37928028287814\n",
      "Average reward: 198.46031626873423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:44:47,048] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001560.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 198.5373004552975\n",
      "Average reward: 198.61043543253263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 197.51091797786066\n",
      "Average reward: 197.6353720789676\n",
      "Average reward: 197.75360347501922\n",
      "Average reward: 197.21262713620482\n",
      "Average reward: 196.33717619090362\n",
      "Average reward: 196.52031738135844\n",
      "Average reward: 196.6943015122905\n",
      "Average reward: 193.32946258758778\n",
      "Average reward: 193.6629894582084\n",
      "Average reward: 193.97983998529796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:45:03,077] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001580.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 194.28084798603305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 193.5559653073948\n",
      "Average reward: 193.87816704202507\n",
      "Average reward: 194.1842586899238\n",
      "Average reward: 194.4750457554276\n",
      "Average reward: 194.7512934676562\n",
      "Average reward: 195.0137287942734\n",
      "Average reward: 195.2630423545597\n",
      "Average reward: 195.3448957249901\n",
      "Average reward: 195.13376839180356\n",
      "Average reward: 195.3232259736027\n",
      "Average reward: 195.55706467492254\n",
      "Average reward: 195.77921144117641\n",
      "Average reward: 195.99025086911757\n",
      "Average reward: 196.1907383256617\n",
      "Average reward: 196.3812014093786\n",
      "1600\n",
      "Average reward: 196.56214133890967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:45:19,378] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001600.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.73403427196416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 196.89733255836595\n",
      "Average reward: 197.05246593044765\n",
      "Average reward: 197.19984263392527\n",
      "Average reward: 197.339850502229\n",
      "Average reward: 197.47285797711754\n",
      "Average reward: 197.10175432434858\n",
      "Average reward: 197.24666660813114\n",
      "Average reward: 197.38433327772458\n",
      "Average reward: 197.51511661383833\n",
      "Average reward: 197.6393607831464\n",
      "Average reward: 197.75739274398907\n",
      "Average reward: 197.86952310678961\n",
      "Average reward: 197.97604695145012\n",
      "Average reward: 198.0772446038776\n",
      "Average reward: 198.17338237368372\n",
      "Average reward: 198.26471325499952\n",
      "Average reward: 198.35147759224952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 16:45:35,283] Starting new video recorder writing to /datasets/home/86/786/dorozco/DQN/openaigym.video.2.202.video001620.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 198.43390371263703\n",
      "Average reward: 198.51220852700519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 198.58659810065492\n",
      "Average reward: 198.65726819562218\n",
      "Average reward: 198.72440478584107\n",
      "Average reward: 198.78818454654902\n",
      "Average reward: 198.84877531922155\n",
      "Average reward: 198.90633655326045\n",
      "Average reward: 198.96101972559742\n",
      "199.01296873931756\n"
     ]
    }
   ],
   "source": [
    " # import your DQN and format your average returns as defined above\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "# Create the CartPole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "term_condition = 199\n",
    "class QNetwork(nn.Module):\n",
    "# Define your network here   \n",
    "    def __init__(self,learning_rate = 0.01, state_size=4, action_size=2, hidden_size=20, alpha_decay=0.01):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size,bias=True)\n",
    "        #torch.nn.init.xavier_uniform(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        #torch.nn.init.xavier_uniform(self.fc2.weight)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size,bias=True)\n",
    "        #torch.nn.init.xavier_uniform(self.fc3.weight)\n",
    "        #initialize loss criterion\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #x = self.fc3(x)\n",
    "        return x\n",
    "        #initilize an optimizer\n",
    "    def initialize(self):\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        self.schedule = optim.lr_scheduler.ExponentialLR(self.optimizer,self.alpha_decay)\n",
    "    def copy(self,net):\n",
    "        self.load_state_dict(net.state_dict())\n",
    "        \n",
    "\n",
    "DQN = QNetwork()\n",
    "DQN.initialize()\n",
    "DQNc = QNetwork()\n",
    "DQNc.copy(DQN)\n",
    "\n",
    "\n",
    "print (DQN)\n",
    "print (DQNc)\n",
    "\n",
    "class Replay():\n",
    "    def __init__(self,max_size):\n",
    "        self.max_size = max_size\n",
    "        self.replay_buffer = []\n",
    "        \n",
    "    def initialize(self,init_length,env):\n",
    "        observation = env.reset()\n",
    "        for steps in range(init_length):\n",
    "            #time.sleep(0.05)\n",
    "            #env.render()\n",
    "            experience = []\n",
    "            observation = np.reshape(observation, [1, observation.size])\n",
    "            experience.append(observation)\n",
    "            action = env.action_space.sample()\n",
    "            experience.append(action)\n",
    "            \n",
    "            observation,reward,done,_= env.step(action)\n",
    "                #env.action_space.sample()\n",
    "            experience.append(reward)\n",
    "            observation = np.reshape(observation, [1, observation.size])\n",
    "            experience.append(observation)\n",
    "            experience.append(done)\n",
    "            self.replay_buffer.append(experience)\n",
    "            \n",
    "            if done == True:\n",
    "                observation = env.reset()\n",
    "                \n",
    "    def add_memory(self,memory):\n",
    "        if len(self.replay_buffer) > self.max_size:\n",
    "            self.replay_buffer.pop(0)\n",
    "            self.replay_buffer.append(memory)\n",
    "        else:\n",
    "            self.replay_buffer.append(memory)\n",
    "            \n",
    "    def generate_minibatch(self,DQN,targetDQN,batch_size,discount_factor = 0.99):\n",
    "        states = []\n",
    "        targets = []\n",
    "        dataset_end = len(self.replay_buffer)\n",
    "        #start = np.random.randint(dataset_end-batch_size)\n",
    "        for step in range(batch_size):\n",
    "            step = np.random.randint(dataset_end)\n",
    "            state = (self.replay_buffer[step][0])\n",
    "            \n",
    "            states.append(np.reshape(state,state.size))\n",
    "            state = Variable(torch.from_numpy(state).float())\n",
    "            next_state = self.replay_buffer[step][3]\n",
    "            next_state = np.reshape(next_state, [1, next_state.size])\n",
    "            next_state = Variable(torch.from_numpy(next_state).float())\n",
    "            y = [0.,0.]\n",
    "            action = self.replay_buffer[step][1]\n",
    "            if action == 0:\n",
    "                opposite_action =1\n",
    "            elif action ==1:\n",
    "                opposite_action = 0\n",
    "            else:\n",
    "                print (\"error something went wrong\")\n",
    "            #pposite_action = (action-1)*(-1)\n",
    "            \n",
    "            DQNs = DQN(state).data[0][opposite_action]\n",
    "            \n",
    "            y[opposite_action] = DQNs\n",
    "            if self.replay_buffer[step][4]:\n",
    "                y[action] = (self.replay_buffer[step][2])\n",
    "            else:\n",
    "                future = (discount_factor*torch.max(targetDQN(next_state)).data[0])\n",
    "                y[action] = (self.replay_buffer[step][2]+future)\n",
    "            targets.append(y)\n",
    "        states = np.asarray(states)\n",
    "        targets = np.asarray(targets)\n",
    "        #print np.shape(states)\n",
    "        #print np.shape(targets)\n",
    "        return states,targets\n",
    "# Replay should also have an initialize method which creates a minimum buffer for \n",
    "# the initial episodes to generate minibatches.  \n",
    "\n",
    "\n",
    "#constants\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "# Initialize DQN params\n",
    "learning_rate = 0.01\n",
    "alpha_decay = 0.99\n",
    "hidden_size = 50\n",
    "avg_reward = 0\n",
    "# Play around with your learning rate, alpha decay and hidden layer units \n",
    "\n",
    "# Two layers with a small number of units should be enough\n",
    "DQN = QNetwork(learning_rate, state_size, action_size, hidden_size, alpha_decay)\n",
    "DQN.initialize()\n",
    "targetDQN = QNetwork(learning_rate, state_size, action_size, hidden_size, alpha_decay)\n",
    "#targetDQN.initialize()\n",
    "\n",
    "# set targetDQN weights to DQN weights\n",
    "targetDQN.copy(DQN)\n",
    "# for ex. targetDQN.model.weights = DQN.model.weights (syntax given here is for representation purpose only)\n",
    "\n",
    "## Initialize Replay Buffer\n",
    "###################################\n",
    "## Populate the initial experience buffer\n",
    "###################################\n",
    "\n",
    "replay = Replay(max_size=10000)\n",
    "replay.initialize(init_length=1000, env=env)\n",
    "\n",
    "# Runtime parameters\n",
    "num_episodes = 2000           # max number of episodes to learn from\n",
    "gamma = 0.99                   # future reward discount\n",
    "max_steps = 500                # cut off simulation after this many steps\n",
    "batch_size = 64\n",
    "# Exploration parameters\n",
    "min_epsilon = 0.01          # minimum exploration probability\n",
    "decay_rate = 5./num_episodes    # exponential decay rate for exploration prob\n",
    "returns = np.zeros(num_episodes)\n",
    "totalSteps = 0\n",
    "running_rewards_DQN = []\n",
    "avg_rewards_DQN = []\n",
    "step_list_DQN = []\n",
    "total_steps = 0\n",
    "logging_interval = 20\n",
    "VISUALIZE = True\n",
    "done_recording = False\n",
    "logdir='./DQN/'\n",
    "if VISUALIZE:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = gym.wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%logging_interval==0)\n",
    "#env._max_episodes_steps = MAX_PATH_LENGTH\n",
    "\n",
    "\n",
    "for ep in range(1, num_episodes):\n",
    "    animate_this_episode = (ep % logging_interval == 0) and VISUALIZE\n",
    "    epsilon = min_epsilon + (1.0 - min_epsilon)*np.exp(-decay_rate*ep)\n",
    "    #print epsilon\n",
    "    if ep%50 == 0:\n",
    "        print (ep)\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    itr = 0\n",
    "    steps = 0\n",
    "    while not done and itr < max_steps:\n",
    "        if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "        itr+=1\n",
    "        experience = []\n",
    "        observation = np.reshape(observation, [1, observation.size])\n",
    "        experience.append(observation)\n",
    "        observation = Variable(torch.from_numpy(observation).float())\n",
    "        if np.random.binomial(1,epsilon):\n",
    "            action = env.action_space.sample()\n",
    "            #print action\n",
    "        else:\n",
    "            action = np.argmax(DQN(observation).data.numpy())\n",
    "            \n",
    "            #print DQN(observation)\n",
    "            #print \"not taking random\"\n",
    "        experience.append(action)\n",
    "            \n",
    "        observation,reward,done,_= env.step(action)\n",
    "                #env.action_space.sample()\n",
    "        experience.append(reward)\n",
    "        observation = np.reshape(observation, [1, observation.size])\n",
    "        experience.append(observation)\n",
    "        experience.append(done)\n",
    "        total_reward+=reward\n",
    "        \n",
    "        replay.add_memory(experience)\n",
    "        total_steps +=1\n",
    "    # --> start episode \n",
    "    # explore/exploit and get action using DQN\n",
    "    # perform action and record new_state, action, reward\n",
    "    # populate Replay experience buffer\n",
    "    # <-- end episode\n",
    "    \n",
    "    returns[ep] = total_reward\n",
    "    if itr == 0:\n",
    "        avg_reward = total_reward\n",
    "    else:\n",
    "        avg_reward = avg_reward * 0.95 + 0.05 * total_reward\n",
    "    \n",
    "    if avg_reward >= term_condition:\n",
    "        print(avg_reward)\n",
    "        break\n",
    "    \n",
    "    total_steps += steps\n",
    "    avg_rewards_DQN.append(avg_reward)\n",
    "    step_list_DQN.append(total_steps)\n",
    "    if itr % logging_interval == 0:\n",
    "        print('Average reward: {}'.format(avg_reward))\n",
    "        \n",
    "    running_rewards_DQN.append(total_reward)\n",
    "    \n",
    "    # Replay\n",
    "    for epoch in range(1):\n",
    "        DQN.schedule.step()\n",
    "        states,targets = replay.generate_minibatch(DQN, targetDQN, batch_size)\n",
    "        targetDQN.copy(DQN)\n",
    "    \n",
    "        target =  Variable(torch.from_numpy(targets).float())\n",
    "        output = DQN(Variable(torch.from_numpy(states).float()))\n",
    "        loss = DQN.criterion(output,target)\n",
    "        DQN.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        DQN.optimizer.step()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(step_list_reinforce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 36, 56, 106, 116, 141, 163, 183, 195, 234, 249, 278, 300, 315, 354, 367, 378, 405, 422, 438, 456, 469, 502, 526, 549, 594, 611, 638, 676, 710, 723, 747, 779, 802, 814, 841, 860, 874, 894, 922, 940, 967, 989, 1004, 1020, 1033, 1047, 1065, 1084, 1111, 1127, 1141, 1150, 1176, 1226, 1239, 1273, 1301, 1315, 1325, 1343, 1357, 1379, 1390, 1424, 1458, 1477, 1491, 1505, 1517, 1527, 1537, 1548, 1573, 1593, 1605, 1626, 1653, 1665, 1702, 1713, 1731, 1758, 1768, 1781, 1797, 1812, 1826, 1837, 1855, 1880, 1924, 1945, 1985, 2039, 2064, 2114, 2125, 2172, 2198, 2216, 2262, 2273, 2296, 2320, 2339, 2371, 2386, 2403, 2431, 2447, 2475, 2494, 2524, 2549, 2571, 2586, 2608, 2627, 2654, 2663, 2676, 2699, 2722, 2734, 2753, 2768, 2789, 2800, 2828, 2858, 2879, 2894, 2910, 2923, 2972, 2984, 3100, 3130, 3158, 3172, 3201, 3216, 3282, 3321, 3336, 3385, 3413, 3491, 3548, 3608, 3631, 3685, 3696, 3716, 3861, 3896, 3913, 3935, 3958, 3971, 3992, 4035, 4065, 4099, 4125, 4163, 4176, 4211, 4242, 4263, 4311, 4335, 4381, 4470, 4491, 4537, 4573, 4648, 4674, 4714, 4746, 4783, 4838, 4874, 4914, 4931, 4979, 4992, 5020, 5038, 5116, 5158, 5185, 5232, 5281, 5308, 5410, 5429, 5446, 5512, 5550, 5604, 5675, 5797, 5837, 5896, 5987, 6007, 6061, 6090, 6146, 6166, 6242, 6267, 6343, 6370, 6391, 6443, 6473, 6493, 6531, 6579, 6649, 6732, 6804, 6825, 6877, 6901, 6955, 7052, 7095, 7131, 7226, 7315, 7427, 7446, 7465, 7568, 7596, 7638, 7671, 7771, 7804, 7908, 7927, 8031, 8129, 8167, 8367, 8399, 8487, 8512, 8617, 8633, 8693, 8804, 8917, 8955, 9081, 9210, 9330, 9374, 9535, 9653, 9788, 9908, 9962, 10063, 10148, 10165, 10212, 10330, 10422, 10444, 10459, 10565, 10584, 10716, 10734, 10837, 10853, 10920, 11049, 11070, 11112, 11149, 11344, 11518, 11558, 11680, 11824, 12007, 12104, 12191, 12237, 12417, 12528, 12673, 12873, 12935, 13087, 13236, 13359, 13527, 13584, 13768, 13966, 14101, 14126, 14282, 14433, 14547, 14693, 14785, 14850, 14871, 14968, 15106, 15207, 15304, 15425, 15520, 15634, 15645, 15694, 15787, 15847, 15994, 16028, 16120, 16235, 16330, 16384, 16451, 16543, 16569, 16668, 16770, 16887, 16975, 17088, 17195, 17309, 17397, 17495, 17602, 17713, 17850, 17941, 18059, 18182, 18232, 18360, 18395, 18504, 18621, 18636, 18662, 18791, 18908, 19065, 19173, 19293, 19417, 19617, 19731, 19820, 19922, 19987, 20012, 20135, 20252, 20277, 20409, 20517, 20626, 20651, 20752, 20889, 20943, 20963, 21098, 21215, 21287, 21384, 21481, 21641, 21734, 21845, 21919, 22038, 22153, 22320, 22441, 22579, 22591, 22608, 22742, 22796, 22955, 23090, 23225, 23377, 23499, 23629, 23693, 23799, 23955, 24079, 24279, 24400, 24547, 24700, 24900, 25100, 25125, 25266, 25458, 25658, 25858, 26058, 26258, 26458, 26658, 26858, 27058, 27258, 27458, 27658, 27858, 28058, 28258, 28441, 28641, 28710, 28738, 28938, 29077, 29277, 29288, 29488, 29688, 29865, 29924, 30109, 30282, 30482, 30657, 30857, 31057, 31244, 31444, 31604, 31699, 31899, 32099, 32299, 32499, 32684, 32884, 33054, 33254, 33313, 33489, 33681, 33878, 34078, 34278, 34478, 34678, 34878, 35078, 35241, 35441, 35605, 35805, 35984, 36184, 36371, 36510, 36646, 36824, 36950, 37065, 37265, 37465, 37665, 37823, 38021, 38221, 38421, 38591, 38791, 38961, 39130, 39298, 39454, 39651, 39851, 40042, 40201, 40359, 40538, 40711, 40895, 41079, 41279, 41479, 41650, 41823, 41987, 42124, 42244, 42433, 42633, 42833, 43025, 43225, 43369, 43568, 43719, 43868, 43948, 44140, 44340, 44536, 44736, 44816, 44994, 45194, 45384, 45563, 45754, 45954, 46132, 46332, 46525, 46681, 46836, 46905, 47097, 47234, 47400, 47554, 47708, 47834, 48024, 48199, 48358, 48558, 48722, 48922, 49118, 49284, 49445, 49594, 49793, 49993, 50193, 50342, 50542, 50688, 50888, 51056, 51221, 51421, 51611, 51811, 51998, 52198, 52398, 52598, 52798, 52970, 53170, 53332, 53532, 53694, 53842, 54042, 54242, 54424, 54624, 54818, 55002, 55189, 55260, 55460, 55660, 55859, 56059, 56257, 56422, 56577, 56717, 56864, 57063, 57238, 57394, 57565, 57727, 57915, 58115, 58288, 58488, 58688, 58812, 58993, 59189, 59389, 59568, 59752, 59914, 60114, 60314, 60504, 60704, 60858, 61058, 61258, 61458, 61658, 61858, 62048, 62248, 62448, 62648, 62840, 63040, 63240, 63421, 63621, 63821, 63906, 64106, 64306, 64506, 64706, 64906, 65106, 65267, 65439, 65639, 65839, 66039, 66216, 66393, 66593, 66793, 66987, 67187, 67271, 67452, 67652, 67852, 68007, 68207, 68407, 68607, 68807, 69007, 69207, 69407, 69607, 69788, 69973, 70173, 70354, 70554, 70754, 70954, 71122, 71322, 71522, 71697, 71878, 72078, 72278, 72478, 72678, 72878, 73078, 73231, 73410, 73610, 73810, 73976, 74176, 74365, 74565, 74765, 74965, 75164, 75364, 75554, 75754, 75935, 76135, 76335, 76535, 76735, 76935, 77135, 77335, 77535, 77717, 77917, 78117, 78317, 78517, 78711, 78898, 79085, 79285, 79485, 79685, 79885, 80085, 80278, 80478, 80678, 80878, 81071, 81251, 81451, 81651, 81851, 82028, 82191, 82382, 82582, 82782, 82982, 83182, 83362, 83562, 83755, 83955, 84155, 84309, 84509, 84709, 84878, 85070, 85270, 85470, 85670, 85868, 86038, 86238, 86409, 86609, 86809, 87009, 87209, 87409, 87609, 87809, 87993, 88152, 88352, 88552, 88752, 88924, 89122, 89319, 89519, 89719, 89919, 90092, 90292, 90492, 90683, 90871, 91071, 91271, 91471, 91671, 91871, 92033, 92233, 92433, 92633, 92833, 93033, 93211, 93411, 93611, 93811, 94011, 94211, 94411, 94598, 94798, 94993, 95193, 95393, 95593, 95793, 95993, 96193, 96370, 96570, 96770, 96970, 97146, 97346, 97546, 97746, 97946, 98146, 98300, 98500, 98700, 98900, 99100, 99300, 99478, 99640, 99840, 100034, 100234, 100434, 100634, 100834, 101034, 101234, 101434, 101634, 101790, 101990, 102190, 102390, 102569, 102769, 102969, 103169, 103369, 103569, 103746, 103930, 104130, 104330, 104530, 104704, 104904, 105104, 105304, 105490, 105690, 105890, 106090, 106290, 106490, 106690, 106890, 107090, 107290, 107468, 107668, 107864, 108064, 108252, 108452, 108652, 108847, 109047, 109247, 109447, 109647, 109847, 110047, 110247, 110447, 110647, 110847, 111047, 111247, 111440, 111619, 111819, 112019, 112219, 112419, 112619, 112819, 113019, 113219, 113419, 113619, 113819, 114019, 114219, 114387, 114587, 114787, 114972, 115162, 115322, 115490, 115690, 115858, 116058, 116258, 116458, 116658, 116858, 117058, 117258, 117458, 117658, 117857, 118057, 118257, 118457, 118657, 118857, 119057, 119257, 119457, 119653, 119853, 120053, 120253, 120453, 120653, 120853, 121030, 121230, 121430, 121630, 121830, 122030, 122230, 122430, 122630, 122830, 123030, 123230, 123430, 123630, 123820, 124018, 124218, 124394, 124558, 124758, 124944, 125144, 125344, 125544, 125744, 125944, 126144, 126344, 126544, 126744, 126944, 127144, 127344, 127544, 127744, 127943, 128127, 128303, 128503, 128697, 128897, 129097, 129297, 129497, 129697, 129897, 130097, 130297, 130497, 130697, 130897, 131091, 131291, 131491, 131691, 131891, 132091, 132291, 132491, 132662, 132862, 133062, 133260, 133460, 133660, 133850, 134050, 134248, 134448, 134648, 134848, 135048, 135234, 135434, 135634, 135834, 136017, 136217, 136399, 136599, 136799, 136984, 137184, 137384, 137570, 137758, 137958, 138158, 138358, 138542, 138741, 138941, 139141, 139341, 139538, 139738, 139910, 140110, 140273, 140473, 140673, 140873, 141073, 141259, 141459, 141659, 141844, 142044, 142244, 142444, 142614, 142814, 143014, 143208, 143408, 143608, 143808, 144008, 144208, 144408, 144575, 144775, 144975, 145175, 145375, 145544, 145744, 145944, 146144, 146344, 146544, 146744, 146944, 147144, 147344, 147544, 147744, 147944, 148144, 148334, 148508, 148708, 148908, 149088, 149288, 149488, 149688, 149888, 150088, 150288, 150487, 150672, 150872, 151072, 151272, 151447, 151647, 151847, 152039, 152050, 152250, 152450, 152650, 152850, 153050, 153250, 153450, 153638, 153838, 154038, 154238, 154438, 154613, 154798, 154998, 155198, 155398, 155598, 155790, 155990, 156168, 156368, 156566, 156758, 156958, 157158, 157335, 157535, 157735, 157909, 158079, 158279, 158479, 158679, 158879, 159079, 159279, 159457, 159652, 159852, 160052, 160252, 160452, 160652, 160824, 161024, 161197, 161397, 161597, 161797, 161958, 162158, 162358, 162558, 162758, 162958, 163132, 163332, 163532, 163732, 163932, 164132, 164332, 164532, 164732, 164901, 165101, 165301, 165501, 165668, 165868, 166046, 166246, 166446, 166633, 166833, 167033, 167233, 167433, 167633, 167833, 168024, 168224, 168424, 168624, 168824, 169024, 169224, 169424, 169610, 169810, 170010, 170210, 170410, 170604, 170804, 171004, 171204, 171402, 171602, 171777, 171977, 172177, 172377, 172567, 172745, 172945, 173145, 173345, 173545, 173745, 173945, 174145, 174345, 174509, 174675, 174875, 175074, 175274, 175474, 175674, 175874, 176053, 176253, 176425, 176625, 176825, 177025, 177225, 177425, 177625, 177825, 178025, 178213, 178413, 178596, 178796, 178996, 179186, 179386, 179581, 179781, 179981, 180181, 180381, 180581, 180781, 180981, 181181, 181381, 181571, 181769, 181931, 182131, 182329, 182523, 182723, 182923, 183123, 183323, 183523, 183723, 183923, 184123, 184323, 184523, 184706, 184900, 185100, 185300, 185482, 185682, 185882, 186082, 186282, 186482, 186669, 186869, 187057, 187257, 187457, 187657, 187857, 188057, 188257, 188457, 188657, 188857, 189057, 189257, 189457, 189647, 189847, 190047, 190247, 190447, 190647, 190847, 191047, 191247, 191447, 191647, 191847, 192047, 192247, 192429, 192629, 192802, 193002, 193202, 193402, 193596, 193796, 193996, 194196, 194396, 194595, 194795, 194995, 195195, 195364, 195564, 195764, 195942, 196142, 196342, 196542, 196742, 196942, 197142, 197342, 197542, 197740, 197940, 198140, 198340, 198540, 198740, 198940, 199118, 199312, 199512, 199712, 199912, 200112, 200312, 200512, 200712, 200898, 201098, 201284, 201484, 201684, 201884, 202084, 202284, 202484, 202671, 202871, 203071, 203271, 203471, 203671, 203871, 204071, 204271, 204466, 204666, 204866, 205066, 205260, 205460, 205660, 205860, 206045, 206245, 206445, 206645, 206845, 207045, 207233, 207433, 207633, 207831, 208031, 208226, 208426, 208626, 208797, 208997, 209197, 209397, 209597, 209773, 209973, 210173, 210373, 210573, 210773, 210973, 211173, 211373, 211573, 211773, 211973, 212173, 212373, 212573, 212773, 212973, 213173, 213357, 213531, 213731, 213905, 214105, 214305, 214505, 214705, 214905, 215105, 215305, 215505, 215685, 215885, 216085, 216285, 216485, 216685, 216885, 217085, 217285, 217485, 217685, 217885, 218085, 218285, 218485, 218685, 218885, 219085, 219285, 219485, 219685, 219869, 220063, 220263, 220463, 220663, 220845, 221009, 221209, 221409, 221609, 221784, 221984, 222184, 222372, 222572, 222772, 222972, 223172, 223372, 223572, 223772, 223972, 224172, 224372, 224568, 224768, 224968, 225142, 225342, 225542, 225742, 225942, 226142, 226342, 226542, 226719, 226919, 227119, 227319, 227519, 227719, 227919, 228119, 228319, 228519, 228709, 228909, 229109, 229309, 229509, 229709, 229881, 230081, 230281, 230481, 230681, 230875, 231065, 231265, 231465, 231665, 231865, 232065, 232265, 232465, 232665, 232865, 233065, 233265, 233465, 233665, 233865, 234065, 234265, 234465, 234665, 234865, 235065, 235265, 235445, 235645, 235845, 236045, 236245, 236440, 236640, 236840, 237020, 237215, 237415, 237615, 237815, 238015, 238179, 238379, 238579, 238771, 238971, 239171, 239371, 239571, 239771, 239971, 240150, 240350, 240550, 240750, 240950, 241140, 241340, 241540, 241740, 241940, 242137, 242337, 242537, 242737, 242937, 243137, 243333, 243533, 243733, 243933, 244133, 244333, 244533, 244733, 244933, 245133, 245307, 245507, 245707, 245907, 246091, 246291, 246467, 246663, 246863, 247063, 247263, 247440, 247615, 247781, 247969, 248169, 248369, 248569, 248769, 248942, 249142, 249342, 249542, 249742, 249942, 250142, 250342, 250534, 250734, 250920, 251120, 251314, 251514, 251714, 251914, 252114, 252314, 252514, 252714, 252914, 253114, 253314, 253514, 253714, 253914, 254101, 254301, 254501, 254701, 254901, 255101, 255301, 255501, 255701, 255901, 256101, 256301, 256501, 256701, 256901, 257101, 257301, 257501, 257701, 257901, 258101, 258301]\n"
     ]
    }
   ],
   "source": [
    "print(step_list_DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "#out = numpy_ewma_vectorized_v2(np.array(running_rewards_DQN),20)\n",
    "#plt.plot(step_list_ddpg, out) # or \n",
    "plt.plot(step_list_DQN, running_rewards_DQN)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "#plt.legend(['DDPG', 'REINFORCE']) #or \n",
    "plt.legend(['DQN', 'REINFORCE'])\n",
    "plt.plot(step_list_reinforce, avg_rewards_reinforce)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
