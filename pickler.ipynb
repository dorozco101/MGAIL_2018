{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "envName = 'Hopper-v1'\n",
    "#make sure we adjust the environment name because we dont want to overwrite!!!\n",
    "path = 'expert_pickles/'+envName+'.pickle'\n",
    "file = open(path,'rb')\n",
    "print(\"okay...\")\n",
    "traj = pickle.load(file)\n",
    "print(\"starting\")\n",
    "episodes = []\n",
    "states = []\n",
    "actions = []\n",
    "nextStates = []\n",
    "rewards = []\n",
    "terminals = []\n",
    "qpos=[]\n",
    "qvel=[]\n",
    "rewardCounter = 0\n",
    "\n",
    "lowerLim = 2400\n",
    "upperLim = 3500\n",
    "upperLim2 = upperLim\n",
    "traj = traj[87000:1000000]+traj[6700000:6800000]\n",
    "print(\"actually starting\")\n",
    "for step in traj:\n",
    "    states.append(step[0])\n",
    "    actions.append(step[1])\n",
    "    reward = step[2]\n",
    "    rewards.append(reward)\n",
    "    nextStates.append(step[3])\n",
    "    done = step[4]\n",
    "    terminals.append(done)\n",
    "    qpos.append(step[5])\n",
    "    qvel.append(step[6])\n",
    "    rewardCounter += reward\n",
    "    if done:\n",
    "        episode=[]\n",
    "        episode.append(states)\n",
    "        episode.append(actions)\n",
    "        episode.append(rewards)\n",
    "        episode.append(nextStates)\n",
    "        episode.append(terminals)\n",
    "        episode.append(qpos)\n",
    "        episode.append(qvel)\n",
    "        episode.append(rewardCounter)\n",
    "        episodes.append(episode)\n",
    "        \n",
    "        #reset everything\n",
    "        states = []\n",
    "        actions = []\n",
    "        nextStates = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        qpos=[]\n",
    "        qvel=[]\n",
    "        rewardCounter = 0\n",
    "    \n",
    "episodes.sort(key=lambda x:x[7])\n",
    "\n",
    "ep_list = list(zip(*episodes))[7]\n",
    "#print(ep_list)\n",
    "counter = 0\n",
    "plt.hist(ep_list)\n",
    "counter = 0 \n",
    "good_indices = []\n",
    "bad_indices = []\n",
    "\n",
    "for index,ep in enumerate(ep_list):\n",
    "    if ep > upperLim:\n",
    "        good_indices.append(index)\n",
    "    if ep < upperLim2 and ep > lowerLim:\n",
    "        bad_indices.append(index)\n",
    "print(len(good_indices))\n",
    "print(len(bad_indices))   \n",
    "\n",
    "\n",
    "\n",
    "good_actions = []\n",
    "good_rewards = []\n",
    "good_terminals = []\n",
    "good_qpos = []\n",
    "good_qvel = []\n",
    "good_nextStates = []\n",
    "\n",
    "mixed_actions = []\n",
    "mixed_rewards = []\n",
    "mixed_terminals = []\n",
    "mixed_qpos = []\n",
    "mixed_qvel = []\n",
    "mixed_nextStates = []\n",
    "\n",
    "bad_actions = []\n",
    "bad_rewards = []\n",
    "bad_terminals = []\n",
    "bad_qpos = []\n",
    "bad_qvel = []\n",
    "bad_nextStates = []\n",
    "\n",
    "broke = False\n",
    "for index in reversed(good_indices):\n",
    "    currentEp = episodes[index]\n",
    "    good_actions = good_actions + currentEp[1]\n",
    "    good_rewards = good_rewards + currentEp[2]\n",
    "    good_nextStates = good_nextStates + currentEp[3]\n",
    "    good_terminals = good_terminals + currentEp[4]\n",
    "    good_qpos = good_qpos+currentEp[5]\n",
    "    good_qvel = good_qvel+currentEp[6]\n",
    "    \n",
    "    if len(good_actions) >= 50000:\n",
    "        print(\"'Good' thresh: Broken correctly\")\n",
    "        broke= True\n",
    "        break\n",
    "    if len(mixed_actions) >= 25000:\n",
    "        pass\n",
    "    else:\n",
    "        mixed_actions = mixed_actions + currentEp[1]\n",
    "        mixed_rewards = mixed_rewards + currentEp[2]\n",
    "        mixed_nextStates = mixed_nextStates + currentEp[3]\n",
    "        mixed_terminals = mixed_terminals + currentEp[4]\n",
    "        mixed_qpos = mixed_qpos+currentEp[5]\n",
    "        mixed_qvel = mixed_qvel+currentEp[6]\n",
    "if broke == False:\n",
    "    print(\"'Error: not enough data in range: change 'good' thresh\")\n",
    "                \n",
    "broke = False     \n",
    "for index in (bad_indices):\n",
    "    currentEp = episodes[index]\n",
    "    bad_actions = bad_actions + currentEp[1]\n",
    "    bad_rewards = bad_rewards + currentEp[2]\n",
    "    bad_nextStates = bad_nextStates + currentEp[3]\n",
    "    bad_terminals = bad_terminals + currentEp[4]\n",
    "    bad_qpos = bad_qpos+currentEp[5]\n",
    "    bad_qvel = bad_qvel+currentEp[6]\n",
    "    \n",
    "    if len(bad_actions) >= 50000:\n",
    "        print(\"'Bad' Thresh: Broken correctly\")\n",
    "        broke= True\n",
    "        break\n",
    "    if len(mixed_actions) >= 50000:\n",
    "        pass\n",
    "    else:\n",
    "        mixed_actions = mixed_actions + currentEp[1]\n",
    "        mixed_rewards = mixed_rewards + currentEp[2]\n",
    "        mixed_nextStates = mixed_nextStates + currentEp[3]\n",
    "        mixed_terminals = mixed_terminals + currentEp[4]\n",
    "        mixed_qpos = mixed_qpos+currentEp[5]\n",
    "        mixed_qvel = mixed_qvel+currentEp[6]\n",
    "if broke == False:\n",
    "    print(\"Error: not enough data in range: change 'bad' thresh\")\n",
    "\n",
    "\n",
    "good_actions=good_actions[:50000]\n",
    "good_rewards=good_rewards[:50000]\n",
    "good_nextStates=good_nextStates[:50000]\n",
    "good_terminals=good_terminals[:50000]\n",
    "good_qpos=good_qpos[:50000]\n",
    "good_qvel=good_qvel[:50000]\n",
    "good_terminals[-1]=1.0\n",
    "\n",
    "mixed_actions=mixed_actions[:50000]\n",
    "mixed_rewards=mixed_rewards[:50000]\n",
    "mixed_nextStates=mixed_nextStates[:50000]\n",
    "mixed_terminals=mixed_terminals[:50000]\n",
    "mixed_qpos=mixed_qpos[:50000]\n",
    "mixed_qvel=mixed_qvel[:50000]\n",
    "mixed_terminals[-1]=1.0\n",
    "\n",
    "bad_actions=bad_actions[:50000]\n",
    "bad_rewards=bad_rewards[:50000]\n",
    "bad_nextStates=bad_nextStates[:50000]\n",
    "bad_terminals=bad_terminals[:50000]\n",
    "bad_qpos=bad_qpos[:50000]\n",
    "bad_qvel=bad_qvel[:50000]\n",
    "bad_terminals[-1]=1.0\n",
    "\n",
    "envName = 'expert_numpys/'+envName \n",
    "plt.hist(good_rewards)\n",
    "plt.hist(mixed_rewards)\n",
    "plt.hist(bad_rewards)\n",
    "np.save(envName+'_good_actions.npy',np.asarray(good_actions))\n",
    "np.save(envName+'_good_rewards.npy',np.asarray(good_rewards))\n",
    "np.save(envName+'_good_nextStates.npy',np.asarray(good_nextStates))\n",
    "np.save(envName+'_good_terminals.npy',np.asarray(good_terminals))\n",
    "np.save(envName+'_good_qpos.npy',np.asarray(good_qpos))\n",
    "np.save(envName+'_good_qvel.npy',np.asarray(good_qvel))\n",
    "\n",
    "np.save(envName+'_bad_actions.npy',np.asarray(bad_actions))\n",
    "np.save(envName+'_bad_rewards.npy',np.asarray(bad_rewards))\n",
    "np.save(envName+'_bad_nextStates.npy',np.asarray(bad_nextStates))\n",
    "np.save(envName+'_bad_terminals.npy',np.asarray(bad_terminals))\n",
    "np.save(envName+'_bad_qpos.npy',np.asarray(bad_qpos))\n",
    "np.save(envName+'_bad_qvel.npy',np.asarray(bad_qvel))\n",
    "\n",
    "np.save(envName+'_mixed_actions.npy',np.asarray(mixed_actions))\n",
    "np.save(envName+'_mixed_rewards.npy',np.asarray(mixed_rewards))\n",
    "np.save(envName+'_mixed_nextStates.npy',np.asarray(mixed_nextStates))\n",
    "np.save(envName+'_mixed_terminals.npy',np.asarray(mixed_terminals))\n",
    "np.save(envName+'_mixed_qpos.npy',np.asarray(mixed_qpos))\n",
    "np.save(envName+'_mixed_qvel.npy',np.asarray(mixed_qvel))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n",
      "(50000, 4)\n",
      "(50000,)\n",
      "(50000, 2)\n",
      "(50000, 2)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "sentiment = '_mixed'\n",
    "actions = np.load(envName+sentiment+'_actions.npy')\n",
    "states = np.load(envName+sentiment+'_nextStates.npy')\n",
    "rewards = np.load(envName+sentiment+'_rewards.npy')\n",
    "qpos = np.load(envName+sentiment+'_qpos.npy')\n",
    "qvel = np.load(envName+sentiment+'_qvel.npy')\n",
    "terminals = np.load(envName+sentiment+'_terminals.npy')\n",
    "print(actions.shape)\n",
    "print(states.shape)\n",
    "print(rewards.shape)\n",
    "print(qpos.shape)\n",
    "print(qvel.shape)\n",
    "print(terminals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
